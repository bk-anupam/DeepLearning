{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "import numpy as np\r\n",
    "from torch import nn\r\n",
    "from torch.nn import functional as F\r\n",
    "from d2l import torch as d2l\r\n",
    "import re\r\n",
    "from collections import Counter\r\n",
    "import rnn_utils2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "corpus, vocab = rnn_utils2.load_corpus_time_machine()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "\r\n",
    "corpus, vocab = load_corpus_time_machine()\r\n",
    "len(corpus), len(vocab)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(170108, 28)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def seq_data_iter_random(corpus, batch_size, num_steps):\r\n",
    "    \"\"\"Generate a minibatch of subsequences using random sampling.\"\"\"\r\n",
    "    # Start with a random offset (inclusive of `num_steps - 1`) to partition a sequence\r\n",
    "    corpus = corpus[np.random.randint(0, num_steps - 1):]\r\n",
    "    print(f\"length of corpus = {len(corpus)}\")\r\n",
    "    # Subtract 1 since we need to account for labels\r\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\r\n",
    "    print(f\"num_subseqs = {num_subseqs}\")\r\n",
    "    # The starting indices for subsequences of length `num_steps`\r\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\r\n",
    "    # In random sampling, the subsequences from two adjacent random\r\n",
    "    # minibatches during iteration are not necessarily adjacent on the\r\n",
    "    # original sequence\r\n",
    "    np.random.shuffle(initial_indices)\r\n",
    "    print(initial_indices)\r\n",
    "\r\n",
    "    def data(pos):\r\n",
    "        # Return a sequence of length `num_steps` starting from `pos`\r\n",
    "        return corpus[pos:pos + num_steps]\r\n",
    "\r\n",
    "    num_batches = num_subseqs // batch_size\r\n",
    "    print(f\"num_batches = {num_batches}\")\r\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\r\n",
    "        # Here, `initial_indices` contains randomized starting indices for subsequences\r\n",
    "        print(f\"i = {i}, batch_size = {batch_size}\")\r\n",
    "        initial_indices_per_batch = initial_indices[i:i + batch_size]\r\n",
    "        print(initial_indices_per_batch)\r\n",
    "        X = [data(j) for j in initial_indices_per_batch]\r\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\r\n",
    "        yield torch.tensor(X), torch.tensor(Y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\r\n",
    "    \"\"\"Generate a minibatch of subsequences using sequential partitioning.\"\"\"\r\n",
    "    # Start with a random offset to partition a sequence\r\n",
    "    offset = np.random.randint(0, num_steps)\r\n",
    "    print(f\"offset = {offset}\")\r\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\r\n",
    "    print(f\"num_tokens = {num_tokens}\")\r\n",
    "    Xs = torch.tensor(corpus[offset:offset + num_tokens])\r\n",
    "    Ys = torch.tensor(corpus[offset + 1:offset + 1 + num_tokens])\r\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\r\n",
    "    print(f\"Xs.shape = {Xs.shape}\")\r\n",
    "    num_batches = Xs.shape[1] // num_steps\r\n",
    "    print(f\"num_batches = {num_batches}\")\r\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\r\n",
    "        X = Xs[:, i:i + num_steps]\r\n",
    "        Y = Ys[:, i:i + num_steps]\r\n",
    "        yield X, Y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "my_seq = list(range(35))\r\n",
    "for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\r\n",
    "    print('X: ', X, '\\nY:', Y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "offset = 0\n",
      "num_tokens = 34\n",
      "Xs.shape = torch.Size([2, 17])\n",
      "num_batches = 3\n",
      "X:  tensor([[ 0,  1,  2,  3,  4],\n",
      "        [17, 18, 19, 20, 21]]) \n",
      "Y: tensor([[ 1,  2,  3,  4,  5],\n",
      "        [18, 19, 20, 21, 22]])\n",
      "X:  tensor([[ 5,  6,  7,  8,  9],\n",
      "        [22, 23, 24, 25, 26]]) \n",
      "Y: tensor([[ 6,  7,  8,  9, 10],\n",
      "        [23, 24, 25, 26, 27]])\n",
      "X:  tensor([[10, 11, 12, 13, 14],\n",
      "        [27, 28, 29, 30, 31]]) \n",
      "Y: tensor([[11, 12, 13, 14, 15],\n",
      "        [28, 29, 30, 31, 32]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('fastai': conda)"
  },
  "interpreter": {
   "hash": "2ff06204c0662b9359ef4233b0e8cfcc016e07736dbe455d1edaa8487878aae2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}