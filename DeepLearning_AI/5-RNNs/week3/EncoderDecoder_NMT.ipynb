{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date translation (sequence to sequence learning) using encoder decoder architecture with LSTMs. Simulates a language translation task \n",
    "Source sequence is date data in different formats\n",
    "Target sequence is date data in machine readable \"yyyy-mm-dd\" format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import babel\n",
    "from babel.dates import format_date\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from sklearn import model_selection\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "Faker.seed(12345)\n",
    "random.seed(12345)\n",
    "\n",
    "# Define format of the data we would like to generate\n",
    "FORMATS = ['short',\n",
    "           'medium',\n",
    "           'long',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'd MMM YYY', \n",
    "           'd MMMM YYY',\n",
    "           'dd MMM YYY',\n",
    "           'd MMM, YYY',\n",
    "           'd MMMM, YYY',\n",
    "           'dd, MMM YYY',\n",
    "           'd MM YY',\n",
    "           'd MMMM YYY',\n",
    "           'MMMM d YYY',\n",
    "           'MMMM d, YYY',\n",
    "           'dd.MM.YY']\n",
    "\n",
    "# change this if you want it to work with another language\n",
    "LOCALES = ['en_US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Config:\n",
    "    RANDOM_STATE_SEED = 42        \n",
    "    BATCH_SIZE = 2048\n",
    "    NUM_WORKERS = 8\n",
    "    NUM_EPOCHS = 75\n",
    "    PRECISION = 16\n",
    "    NUM_FOLDS = 5\n",
    "    FAST_DEV_RUN = False\n",
    "    DEVICE = device\n",
    "    PATIENCE = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_data():\n",
    "    dt = fake.date_object()\n",
    "    human_readable_dt = None\n",
    "    machine_readable_dt = None\n",
    "    try:\n",
    "        human_readable_dt = format_date(dt, random.choice(FORMATS), \"en_US\")\n",
    "        human_readable_dt = human_readable_dt.replace(\",\", \"\")\n",
    "        machine_readable_dt = dt.isoformat()\n",
    "    except AttributeError as e:\n",
    "        return None, None, None\n",
    "    return human_readable_dt, machine_readable_dt, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_date_dataset(num_examples=100):    \n",
    "    dataset = []\n",
    "    for row in range(num_examples):\n",
    "        h_dt, m_dt, dt = generate_date_data()        \n",
    "        dataset.append([h_dt, m_dt])    \n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vocab for both source and target sequences needs to be generated from training data only\n",
    "# to prevent data leakage into the validation sets leading to inflated model accuracy in validation phase\n",
    "def get_source_target_vocab(human_dates, machine_dates):\n",
    "    human_dt_vocab = set()\n",
    "    machine_dt_vocab = set()\n",
    "    for (h_dt, m_dt) in zip(human_dates, machine_dates):\n",
    "        human_dt_vocab.update(tuple(h_dt))\n",
    "        machine_dt_vocab.update(tuple(m_dt))\n",
    "    # char to index dictionary for source sequence        \n",
    "    human_dt_vocab = {value: index for index, value in enumerate(sorted(human_dt_vocab) + ['<unk>', '<pad>', '<sos>', '<eos>'])}\n",
    "    machine_dt_vocab = {value: index for index, value in enumerate(sorted(machine_dt_vocab) + ['<sos>'])}\n",
    "    inv_machine_dt_vocab = dict(enumerate(sorted(machine_dt_vocab)))  \n",
    "    # index to char dictionary for source sequence\n",
    "    inv_human_dt_vocab = dict(enumerate(sorted(human_dt_vocab)))       \n",
    "    return human_dt_vocab, machine_dt_vocab, inv_human_dt_vocab, inv_machine_dt_vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stoi(str, length, vocab, add_sos_token=False):\n",
    "    \"\"\"\n",
    "    Converts all strings in the vocabulary into a list of integers representing the positions of the\n",
    "    input string's characters in the \"vocab\"\n",
    "    \n",
    "    Arguments:\n",
    "    string -- input string, e.g. 'Wed 10 Jul 2007'\n",
    "    length -- the number of time steps you'd like, determines if the output will be padded or cut\n",
    "    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n",
    "    \n",
    "    Returns:\n",
    "    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n",
    "    \"\"\"\n",
    "    #str = str.lower()\n",
    "    str = str.replace(\",\", \"\")\n",
    "    if len(str) > length:\n",
    "        str = str[:length]\n",
    "    unk_index = vocab.get(\"<unk>\")            \n",
    "    char_indexes = [vocab.get(char, unk_index) for char in str]\n",
    "    if add_sos_token:\n",
    "        sos_index = vocab.get(\"<sos>\")        \n",
    "        # We add index corresponding to <sos> token to the start of target date sequence\n",
    "        char_indexes.insert(0, sos_index)\n",
    "    return np.array(char_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training dataframe into kfolds for cross validation. We do this before any processing is done\n",
    "# on the data. We use stratified kfold if the target distribution is unbalanced\n",
    "def strat_kfold_dataframe(df, target_col_name, num_folds=5):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # randomize of shuffle the rows of dataframe before splitting is done\n",
    "    df = df.sample(frac=1, random_state=Config.RANDOM_STATE_SEED).reset_index(drop=True)\n",
    "    # get the target data\n",
    "    y = df[target_col_name].values\n",
    "    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=Config.RANDOM_STATE_SEED)\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n",
    "        df.loc[val_index, \"kfold\"] = fold\n",
    "    return df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h_dt</th>\n",
       "      <th>m_dt</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monday October 29 2001</td>\n",
       "      <td>2001-10-29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17 02 11</td>\n",
       "      <td>2011-02-17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27 Mar 1973</td>\n",
       "      <td>1973-03-27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>March 22 1978</td>\n",
       "      <td>1978-03-22</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25 Feb 1995</td>\n",
       "      <td>1995-02-25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     h_dt        m_dt  kfold\n",
       "0  Monday October 29 2001  2001-10-29      3\n",
       "1                17 02 11  2011-02-17      2\n",
       "2             27 Mar 1973  1973-03-27      0\n",
       "3           March 22 1978  1978-03-22      4\n",
       "4             25 Feb 1995  1995-02-25      2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_date_dataset(120000)\n",
    "# Let us create a dates dataframe that will contain training data of human readable and machine\n",
    "# readable dates\n",
    "df_dates = pd.DataFrame({\"h_dt\": dataset[:, 0], \"m_dt\": dataset[:, 1]})\n",
    "df_dates = strat_kfold_dataframe(df_dates, target_col_name=\"m_dt\", num_folds=5)  \n",
    "df_dates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the text data using text vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts a string (sequence of chars) to a tensor of ints where each int is the \n",
    "# position of the corresponding char in the relevant vocab\n",
    "class StoITensorTransform(object):\n",
    "    def __init__(self, vocab, max_seq_length, add_sos_token=False):\n",
    "        self.vocab = vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.add_sos_token = add_sos_token\n",
    "\n",
    "    def __call__(self, X):\n",
    "        vectorized_str = stoi(X, self.max_seq_length, self.vocab, self.add_sos_token)        \n",
    "        return torch.from_numpy(vectorized_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding the source sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch is the return value of __getitem__ method of the dataset being used. For DateDataset it is h_dt, m_dt\n",
    "def pad_collate(batch):\n",
    "    # we want to pad the h_dt sequences as these can be of variable length.\n",
    "    # h_dt is of shape len(h_dt)\n",
    "    sorted_batch = sorted(batch, key=lambda x:x[0].shape[0], reverse=True)\n",
    "    h_dt_sorted = [x[0] for x in sorted_batch]\n",
    "    h_dt_padded = pad_sequence(h_dt_sorted, batch_first = True, padding_value=0)\n",
    "    # the original length of the padded h_dt sequences\n",
    "    h_dt_len = torch.Tensor([len(x) for x in h_dt_sorted])\n",
    "    # unpadded m_dt sequences    \n",
    "    m_dt = torch.stack([x[1] for x in sorted_batch])        \n",
    "    return h_dt_padded, h_dt_len, m_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom date dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateDataset(Dataset):\n",
    "    def __init__(self, human_fmt_dates, machine_fmt_dates, transform, target_transform):\n",
    "        super().__init__()\n",
    "        self.h_dts = human_fmt_dates\n",
    "        self.m_dts = machine_fmt_dates\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform        \n",
    "\n",
    "    # Returns vectorized form of human format date and its corresponding machine format date\n",
    "    # with elements of the vectorized date being the index of the characters in the corresponding date vocab\n",
    "    def __getitem__(self, index):\n",
    "        h_dt = self.h_dts[index]\n",
    "        m_dt = self.m_dts[index]\n",
    "        if self.transform:\n",
    "            h_dt = self.transform(h_dt)\n",
    "        if self.target_transform:\n",
    "            m_dt = self.target_transform(m_dt)\n",
    "        return h_dt, m_dt\n",
    "\n",
    "    def __len__(self):                \n",
    "        return len(self.h_dts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training and validation data for a fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train and validation data loaders for a specific fold. \n",
    "# X: numpy array of input features\n",
    "# y: numpy array of target labels\n",
    "# fold: fold index for which to create data loaders                                     \n",
    "# kfolds: Array that marks each of the data items as belonging to a specific fold\n",
    "def get_fold_dls(fold, df):\n",
    "    fold += 1                         \n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    val_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "    h_dt_max_len = train_df.h_dt.apply(lambda x: len(x)).max()\n",
    "    h_vocab, m_vocab, inv_h_vocab, inv_m_vocab = get_source_target_vocab(train_df.h_dt, train_df.m_dt)    \n",
    "    # transform to convert human_date and machine_date to one hot encoded forms\n",
    "    transform = StoITensorTransform(h_vocab, h_dt_max_len)\n",
    "    target_transform = StoITensorTransform(m_vocab, len(m_vocab), add_sos_token=True)\n",
    "    ds_train = DateDataset(train_df.h_dt, train_df.m_dt, transform=transform, target_transform=target_transform)\n",
    "    ds_val = DateDataset(val_df.h_dt, val_df.m_dt, transform=transform, target_transform=target_transform)\n",
    "    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, \n",
    "                        num_workers=Config.NUM_WORKERS, collate_fn=pad_collate)\n",
    "    dl_val = DataLoader(ds_val, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS, \n",
    "                        collate_fn=pad_collate)\n",
    "    return dl_train, dl_val, ds_train, ds_val, h_vocab, m_vocab, inv_h_vocab, inv_m_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train, dl_val, ds_train, ds_val, h_vocab, m_vocab, inv_h_vocab, inv_m_vocab = get_fold_dls(0, df_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/what-does-the-scatter-function-do-in-layman-terms/28037/3\n",
    "def one_hot_encode(input, vocab_size):        \n",
    "    #print(f\"input.shape = {input.shape}, vocab_size = {vocab_size}\")\n",
    "    batch_size = input.shape[0]\n",
    "    seq_length = input.shape[1]\n",
    "    input = input.reshape(batch_size, seq_length, 1).to(Config.DEVICE)    \n",
    "    zeros_tensor = torch.zeros(batch_size, seq_length, vocab_size).to(Config.DEVICE)    \n",
    "    return zeros_tensor.scatter_(2, input, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, drop_out, is_bidirect=False):\n",
    "        super().__init__()\n",
    "        # input to lstm is a sequence (of words, of chars, of anything else). The dimensions being \n",
    "        # (batch_size, sequence_length, input_size) if batch_first = True with sequence_length = length of longest sequence in the batch\n",
    "        # where input_size = number of features(cols) in input X. If you use embedding layer, then each word in the\n",
    "        # the sequence is represented by an embedding vector, so input_size = size of the embedding vector. If one\n",
    "        # hot encoding representation is used then input_size = vocab_size with each word represented by a one hot\n",
    "        # vector with size = vocab_size        \n",
    "        self.input_size = input_size\n",
    "        # hidden_size = number of units in the hidden layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.is_bidirect = is_bidirect\n",
    "        self.num_directions = 2 if is_bidirect else 1\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size = input_size, \n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            dropout = drop_out,\n",
    "            bidirectional = is_bidirect\n",
    "            )                \n",
    "\n",
    "    def forward(self, inputs, input_lengths):       \n",
    "        # inputs = [batch_size, max_seq_length] \n",
    "        # we are going to use one hot encoding representation of the human dates data. The input data is\n",
    "        # vectorized form of human format date with elements of the vectorized date being the index of the \n",
    "        # characters in the corresponding date vocab (input_size = vocab_size)\n",
    "        # inputs_oh = F.one_hot(inputs.T.float(), self.input_size)\n",
    "        #print(f\"inputs.shape = {inputs.shape}\")\n",
    "        inputs_oh = one_hot_encode(inputs, self.input_size)\n",
    "        #print(f\"inputs_oh.shape = {inputs_oh.shape}\")\n",
    "        # inputs_oh = [batch_size, max_seq_length, vocab_size]\n",
    "        # pack_padded_sequence before feeding into LSTM. This is required so pytorch knows\n",
    "        # which elements of the sequence are padded ones and ignore them in computation.\n",
    "        packed_padded_inputs = pack_padded_sequence(inputs_oh, input_lengths.to(\"cpu\"), batch_first=True)\n",
    "        lstm_out_pack, (h_final, c_final) = self.lstm_layer(packed_padded_inputs)\n",
    "        # h_final and c_final = [num_direction * num_layers, batch_size, hidden_size]        \n",
    "        return h_final, c_final\n",
    "\n",
    "    # `nn.LSTM` takes a tuple of hidden states (h0, c0). h0 = initial\n",
    "    # hidden state for each element in the batch, c0 = initial cell state for each element in the batch\n",
    "    def init_state(self, batch_size):\n",
    "        return (\n",
    "            torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size)),\n",
    "            torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, drop_out):\n",
    "        super().__init__()        \n",
    "        self.input_size = input_size        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers                \n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size = input_size, \n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            dropout = drop_out,\n",
    "            bidirectional = False\n",
    "            )   \n",
    "        self.linear = nn.Linear(hidden_size, input_size)                        \n",
    "\n",
    "    def forward(self, input_oh, encoder_hidden, encoder_cell):\n",
    "        # input_oh = [batch_size, target_vocab_size]\n",
    "        # The input sequence length in decoder is always 1 as we feed in one character at a time\n",
    "        input_oh = input_oh.unsqueeze(1).to(Config.DEVICE)\n",
    "        # input_oh = [batch_size, 1, target_vocab_size]        \n",
    "        # inputs_oh = F.one_hot(inputs.T.long(), self.input_size)                \n",
    "        lstm_out, (h_final, c_final) = self.lstm_layer(input_oh, (encoder_hidden, encoder_cell))\n",
    "        #print(f\"decoder lstm_out.shape = {lstm_out.shape}\")\n",
    "        # lstm_out = [batch_size , seq_length , num_directions * hidden_size]\n",
    "        # h_final and c_final = [num_direction * num_layers, batch_size, hidden_size]\n",
    "        # seq_length and num_direction will always be 1 for decoder. Thus\n",
    "        # lstm_out = [batch_size, 1, hidden_size]\n",
    "        # h_final and c_final = [num_layers, batch_size, hidden_size]\n",
    "        pred = self.linear(lstm_out.squeeze(1))        \n",
    "        # pred = [batch_size, output_dim] where output_dim = vocab_size of target sequences ( machine dates in our case)\n",
    "        # For multi class classification the number of output nodes is equal to the number of classes to predict (vocab size)\n",
    "        return pred, h_final, c_final\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder decoder model with pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderLitModel(pl.LightningModule):\n",
    "    def __init__(self, hparams, source_vocab_size, target_vocab_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = hparams[\"lr\"]\n",
    "        self.weight_decay = hparams[\"weight_decay\"]\n",
    "        # target_vocab_size = vocab_size for target sequence data (machine_date) as we are using one hot encoding\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.encoder = lstm_encoder(\n",
    "            input_size = source_vocab_size,\n",
    "            hidden_size = hparams[\"hidden_size\"],\n",
    "            num_layers = hparams[\"num_layers\"],\n",
    "            drop_out = hparams[\"enc_drop_out\"]\n",
    "            )\n",
    "        self.decoder = lstm_decoder(\n",
    "            input_size = target_vocab_size,\n",
    "            hidden_size = hparams[\"hidden_size\"],\n",
    "            num_layers = hparams[\"num_layers\"],\n",
    "            drop_out = hparams[\"dec_drop_out\"]\n",
    "        )            \n",
    "\n",
    "    def forward(self, src_seq, src_seq_lengths, target_seq_oh, teacher_forcing_ratio=0.5):        \n",
    "        # src_seq = [batch_size, max_source_seq_length]\n",
    "        # target_seq = [batch_size, target_seq_length]\n",
    "        batch_size = target_seq_oh.shape[0]        \n",
    "        # target sequence length is 11 as it includes the <sos> token at the begining\n",
    "        target_seq_length = target_seq_oh.shape[1]            \n",
    "        # tensor to store decoder output\n",
    "        dec_outputs = torch.zeros((batch_size, target_seq_length, self.target_vocab_size))        \n",
    "        #print(f\"dec_outputs.shape = {dec_outputs.shape}\")\n",
    "        # first input to the decoder is the <sos> token which is the first character in all target sequences\n",
    "        input = target_seq_oh[:, 0, :].reshape(batch_size, -1)\n",
    "        # input = [batch_size, target_vocab_size]\n",
    "        #print(f\"decoder input.shape = {input.shape}\")\n",
    "        # last hidden and cell state of the encoder is used as initial hidden and cell state of the decoder\n",
    "        hidden, cell = self.encoder(src_seq, src_seq_lengths)\n",
    "        for t in range(1, target_seq_length):            \n",
    "            dec_output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # dec_output = [batch_size, target_vocab_size]            \n",
    "            dec_outputs[:, t, :] = dec_output\n",
    "            # whether to use teacher forcing\n",
    "            teacher_forcing = False if np.random.random() < teacher_forcing_ratio else True            \n",
    "            # if teacher forcing use actual token at t as the input to t+1, otherwise use the prediction at t\n",
    "            # as the input to t+1\n",
    "            actual_t = target_seq_oh[:, t, :]\n",
    "            input = actual_t if teacher_forcing else dec_outputs[:, t, :]\n",
    "            input = input.reshape(batch_size, -1)\n",
    "        return dec_outputs            \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, mode=\"min\")\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # data loader batch doesn't perform one hot encoding of either source or target sequences\n",
    "        src_padded_seq, src_seq_lengths, target_seq = batch\n",
    "        # target_seq = [batch_size, target_seq_length]\n",
    "        # src_padded_seq = [batch_size, max_src_seq_length]\n",
    "        target_seq_oh = one_hot_encode(target_seq, self.target_vocab_size)\n",
    "        # target_seq_oh = [batch_size, target_seq_length, target_vocab_size]\n",
    "        pred_target_seq = self(src_padded_seq, src_seq_lengths, target_seq_oh)\n",
    "        # pred_target_seq = [batch_size, target_seq_length, target_vocab_size]\n",
    "        # we will exclude the first character from both the predicted and actual target dates. The first character\n",
    "        # in target_dates in <sos> token while the first value in pred_target_dates is 0.         \n",
    "        #print(f\"target_seq.shape = {target_seq.shape}\")\n",
    "        #print(f\"pred_target_seq.shape = {pred_target_seq.shape}\")\n",
    "        target_seq = target_seq[:, 1:]        \n",
    "        # flatten the target_seq to 1d \n",
    "        target_seq = target_seq.reshape(-1)\n",
    "        pred_target_seq = pred_target_seq[:, 1:, :].to(Config.DEVICE)\n",
    "        # flatten the predicted target seq to 2d\n",
    "        pred_target_seq = pred_target_seq.view(-1, self.target_vocab_size)\n",
    "        #print(f\"target_seq.shape = {target_seq.shape}\")\n",
    "        #print(f\"pred_target_seq.shape = {pred_target_seq.shape}\")                \n",
    "        train_loss = cross_entropy(pred_target_seq, target_seq)\n",
    "        train_perplexity = torch.exp(train_loss)\n",
    "        self.log(\"train_loss\", train_loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_perplexity\", train_perplexity, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):        \n",
    "        src_padded_seq, src_seq_lengths, target_seq = batch\n",
    "        target_seq_oh = one_hot_encode(target_seq, self.target_vocab_size)\n",
    "        # Remember to turn teacher forcing off for validation\n",
    "        pred_target_seq = self(src_padded_seq, src_seq_lengths, target_seq_oh, teacher_forcing_ratio=0)\n",
    "        target_seq = target_seq[:, 1:].reshape(-1)\n",
    "        pred_target_seq = pred_target_seq[:, 1:, :].to(Config.DEVICE)\n",
    "        # flatten the predicted target seq to 2d\n",
    "        pred_target_seq = pred_target_seq.view(-1, self.target_vocab_size)\n",
    "        val_loss = cross_entropy(pred_target_seq, target_seq)\n",
    "        val_perplexity = torch.exp(val_loss)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "        self.log(\"val_perplexity\", val_perplexity, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# For results reproducibility \n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Best trial number = 10\n",
    "# Best trial params:\n",
    "# {'lr': 0.0009729811471218791, 'hidden_size': 218, 'drop_out': 0.20683771027057984, 'num_layers': 2, 'weight_decay': 1.0661805946346311e-06}\n",
    "\n",
    "# model hyperparameters\n",
    "model_params = {    \n",
    "    \"num_layers\": 2,    \n",
    "    \"hidden_size\": 218,\n",
    "    \"enc_drop_out\": 0.206,\n",
    "    \"dec_drop_out\": 0.206,\n",
    "    \"lr\": 0.00097,\n",
    "    \"weight_decay\": 1.066e-06\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom lightning callback \n",
    "To record validation metric values at each epoch and the best metric values across all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "# Monitor multiple metric values that are calculated either in training or validation step and return the\n",
    "# best metric values for each epoch\n",
    "class MetricsAggCallback(Callback):\n",
    "    def __init__(self, metrics_to_monitor):\n",
    "        # dictionary with metric name as key and monitor mode (min, max) as the value\n",
    "        # ( the same names used to log metric values in training and validation step)\n",
    "        self.metrics_to_monitor = metrics_to_monitor\n",
    "        # dictionary with metric_name as key and list of metric value for each epoch\n",
    "        self.metrics = {metric: [] for metric in metrics_to_monitor.keys()}\n",
    "        # dictionary with metric_name as key and the best metric value for all epochs\n",
    "        self.best_metric = {metric: None for metric in metrics_to_monitor.keys()}\n",
    "        # dictionary with metric_name as key and the epoch number with the best metric value\n",
    "        self.best_metric_epoch = {metric: None for metric in metrics_to_monitor.keys()}     \n",
    "        self.epoch_counter = 0   \n",
    "\n",
    "    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):\n",
    "        self.epoch_counter += 1\n",
    "        print(f\"For epoch {self.epoch_counter}\")            \n",
    "        for metric, mode in self.metrics_to_monitor.items():\n",
    "            metric_value = round(trainer.callback_metrics[metric].cpu().detach().item(), 4)            \n",
    "            print(f\"{metric} = {metric_value}\")\n",
    "            self.metrics[metric].append(metric_value)\n",
    "            if mode == \"max\":\n",
    "                self.best_metric[metric] = max(self.metrics[metric])            \n",
    "            elif mode == \"min\":            \n",
    "                self.best_metric[metric] = min(self.metrics[metric])            \n",
    "            self.best_metric_epoch[metric] = self.metrics[metric].index(self.best_metric[metric])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, fold_loss, fold_metrics, dl_train, dl_val, h_vocab, m_vocab, find_lr=True):\n",
    "    fold_str = f\"fold{fold}\"\n",
    "    print(f\"Running training for {fold_str}\")\n",
    "    seq2seq_model = EncoderDecoderLitModel(\n",
    "        hparams = model_params, \n",
    "        source_vocab_size = len(h_vocab),\n",
    "        target_vocab_size = len(m_vocab)\n",
    "        )\n",
    "    tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\")    \n",
    "    chkpt_file_name = \"best_model_{epoch}_{val_loss:.4f}\"\n",
    "    metrics_to_monitor = {\n",
    "        \"val_loss\": \"min\",\n",
    "        \"val_perplexity\": \"min\",\n",
    "        }\n",
    "    loss_chkpt_callback = ModelCheckpoint(dirpath=\"./model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)    \n",
    "    metric_chkpt_callback = MetricsAggCallback(metrics_to_monitor = metrics_to_monitor)\n",
    "    early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=Config.PATIENCE, mode=\"min\", verbose=True)\n",
    "    trainer = pl.Trainer(\n",
    "        gpus = 1,\n",
    "        deterministic = True,\n",
    "        auto_select_gpus = True,\n",
    "        progress_bar_refresh_rate = 20,\n",
    "        max_epochs = Config.NUM_EPOCHS,\n",
    "        logger = tb_logger,\n",
    "        auto_lr_find = True,    \n",
    "        precision = Config.PRECISION,   \n",
    "        fast_dev_run = Config.FAST_DEV_RUN, \n",
    "        gradient_clip_val = 1.0,\n",
    "        #resume_from_checkpoint = \"model/best_model_epoch=71_val_loss=0.4772.ckpt\",\n",
    "        callbacks = [loss_chkpt_callback, metric_chkpt_callback, early_stopping_callback]\n",
    "    )        \n",
    "    if find_lr:\n",
    "        trainer.tune(model=seq2seq_model, train_dataloaders=dl_train)\n",
    "        print(seq2seq_model.lr)\n",
    "    trainer.fit(seq2seq_model, train_dataloaders=dl_train, val_dataloaders=dl_val)\n",
    "    fold_min_loss = loss_chkpt_callback.best_model_score.cpu().detach().item()\n",
    "    fold_loss.append(fold_min_loss)\n",
    "    fold_metrics = {metric: (metric_chkpt_callback.best_metric[metric], metric_chkpt_callback.best_metric_epoch[metric]) \n",
    "                    for metric in metrics_to_monitor.keys()}\n",
    "    print(f\"Best metric value for {fold_str}\")\n",
    "    print(f\"val_loss  = {fold_loss[fold]}\")\n",
    "    print(fold_metrics)\n",
    "    del trainer, seq2seq_model, loss_chkpt_callback, metric_chkpt_callback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "def run_hparam_tuning(model_params, trial):\n",
    "    dl_train, dl_val, ds_train, ds_val, h_vocab, m_vocab, inv_h_vocab, inv_m_vocab = get_fold_dls(0, df_dates)\n",
    "    early_stopping = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")    \n",
    "    seq2seq_model = EncoderDecoderLitModel(\n",
    "        hparams = model_params, \n",
    "        source_vocab_size = len(h_vocab),\n",
    "        target_vocab_size = len(m_vocab)\n",
    "        )    \n",
    "    trainer = pl.Trainer(\n",
    "        checkpoint_callback=False,        \n",
    "        gpus=1,\n",
    "        # For results reproducibility \n",
    "        deterministic=True,\n",
    "        auto_select_gpus=True,\n",
    "        progress_bar_refresh_rate=20,\n",
    "        max_epochs=Config.NUM_EPOCHS,        \n",
    "        precision=Config.PRECISION,   \n",
    "        weights_summary=None,         \n",
    "        gradient_clip_val = 1.0,            \n",
    "        callbacks=[early_stopping]\n",
    "    )      \n",
    "    trainer.fit(seq2seq_model, train_dataloaders=dl_train, val_dataloaders=dl_val)     \n",
    "    loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "    del trainer, seq2seq_model, early_stopping, dl_train, dl_val\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using optuna (uncomment if you want to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# Config.NUM_EPOCHS = 20        \n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"lr\": trial.suggest_loguniform(\"lr\", 1e-6, 1e-3),\n",
    "#         \"hidden_size\": trial.suggest_int(\"hidden_size\", 32, 512),\n",
    "#         \"enc_drop_out\": trial.suggest_uniform(\"enc_drop_out\", 0.2, 0.7),\n",
    "#         \"dec_drop_out\": trial.suggest_uniform(\"dec_drop_out\", 0.2, 0.7),\n",
    "#         \"num_layers\": trial.suggest_int(\"num_layers\", 1, 2),\n",
    "#         \"weight_decay\": trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-2),\n",
    "#     }    \n",
    "#     loss = run_hparam_tuning(params, trial)\n",
    "#     return loss\n",
    "\n",
    "# study = optuna.create_study(direction=\"minimize\", study_name=\"Seq2SeqModelTuning\")    \n",
    "# study.optimize(objective, n_trials=20)\n",
    "# print(f\"Best trial number = {study.best_trial.number}\")\n",
    "# print(\"Best trial params:\")\n",
    "# print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for fold0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ./model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type         | Params\n",
      "-----------------------------------------\n",
      "0 | encoder | lstm_encoder | 613 K \n",
      "1 | decoder | lstm_decoder | 586 K \n",
      "-----------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.803     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa80c82cad449a3a7fee27d27394d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 1\n",
      "val_loss = 2.4783\n",
      "val_perplexity = 11.921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (47) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2923fdfdefd4fca8cbb3bbbbe89f78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104af2fe3448439cbb37cd7218f77b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.617\n",
      "Epoch 0, global step 46: val_loss reached 1.61692 (best 1.61692), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=0_val_loss=1.6169.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 2\n",
      "val_loss = 1.6169\n",
      "val_perplexity = 5.0376\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead53854893f4457b47c780605a9cd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.613 >= min_delta = 0.0. New best score: 1.004\n",
      "Epoch 1, global step 93: val_loss reached 1.00424 (best 1.00424), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=1_val_loss=1.0042.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 3\n",
      "val_loss = 1.0042\n",
      "val_perplexity = 2.7299\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92420d9908324d59b51be02d0b698234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.066 >= min_delta = 0.0. New best score: 0.938\n",
      "Epoch 2, global step 140: val_loss reached 0.93807 (best 0.93807), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=2_val_loss=0.9381.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 4\n",
      "val_loss = 0.9381\n",
      "val_perplexity = 2.555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d00a9e3e65e40a5898a9b7e5751882a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.054 >= min_delta = 0.0. New best score: 0.884\n",
      "Epoch 3, global step 187: val_loss reached 0.88441 (best 0.88441), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=3_val_loss=0.8844.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 5\n",
      "val_loss = 0.8844\n",
      "val_perplexity = 2.4216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132331b4e8554b8b960a246790b297d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.058 >= min_delta = 0.0. New best score: 0.827\n",
      "Epoch 4, global step 234: val_loss reached 0.82668 (best 0.82668), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=4_val_loss=0.8267.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6\n",
      "val_loss = 0.8267\n",
      "val_perplexity = 2.2857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb87bad274b94eb9909d2474ff614ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.058 >= min_delta = 0.0. New best score: 0.769\n",
      "Epoch 5, global step 281: val_loss reached 0.76905 (best 0.76905), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=5_val_loss=0.7691.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 7\n",
      "val_loss = 0.7691\n",
      "val_perplexity = 2.1577\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976bdef7c9744156bc4473902b6c65c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.101 >= min_delta = 0.0. New best score: 0.668\n",
      "Epoch 6, global step 328: val_loss reached 0.66766 (best 0.66766), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=6_val_loss=0.6677.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 8\n",
      "val_loss = 0.6677\n",
      "val_perplexity = 1.9497\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca668a82201f42519e30c1622a718a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.066 >= min_delta = 0.0. New best score: 0.602\n",
      "Epoch 7, global step 375: val_loss reached 0.60206 (best 0.60206), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=7_val_loss=0.6021.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 9\n",
      "val_loss = 0.6021\n",
      "val_perplexity = 1.8259\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215069eaf3fb4ecaac1e6a80943f681d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.031 >= min_delta = 0.0. New best score: 0.571\n",
      "Epoch 8, global step 422: val_loss reached 0.57058 (best 0.57058), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=8_val_loss=0.5706.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 10\n",
      "val_loss = 0.5706\n",
      "val_perplexity = 1.7693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4cd253f52547c68c57430291156fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.083 >= min_delta = 0.0. New best score: 0.487\n",
      "Epoch 9, global step 469: val_loss reached 0.48723 (best 0.48723), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=9_val_loss=0.4872.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 11\n",
      "val_loss = 0.4872\n",
      "val_perplexity = 1.6278\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2cc8e985ea433a9f4b3c66f69c3e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.470\n",
      "Epoch 10, global step 516: val_loss reached 0.47048 (best 0.47048), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=10_val_loss=0.4705.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 12\n",
      "val_loss = 0.4705\n",
      "val_perplexity = 1.6008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17121900d193418f8d0da851ed120846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 563: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 13\n",
      "val_loss = 0.5461\n",
      "val_perplexity = 1.7265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba4cdbcdba146f7bd73648891a8a49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.084 >= min_delta = 0.0. New best score: 0.386\n",
      "Epoch 12, global step 610: val_loss reached 0.38628 (best 0.38628), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=12_val_loss=0.3863.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 14\n",
      "val_loss = 0.3863\n",
      "val_perplexity = 1.4715\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fab67b69634e9db308b1ce974cbd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.385\n",
      "Epoch 13, global step 657: val_loss reached 0.38452 (best 0.38452), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=13_val_loss=0.3845.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 15\n",
      "val_loss = 0.3845\n",
      "val_perplexity = 1.4689\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c465ca7a92ae4cc1ac77e06703e67dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.043 >= min_delta = 0.0. New best score: 0.341\n",
      "Epoch 14, global step 704: val_loss reached 0.34128 (best 0.34128), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=14_val_loss=0.3413.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 16\n",
      "val_loss = 0.3413\n",
      "val_perplexity = 1.4068\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf97cb63a9904299b20d749fb8bad129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.093 >= min_delta = 0.0. New best score: 0.248\n",
      "Epoch 15, global step 751: val_loss reached 0.24815 (best 0.24815), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=15_val_loss=0.2481.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 17\n",
      "val_loss = 0.2481\n",
      "val_perplexity = 1.2817\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3971aca6124d4387c77355a8925c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.035 >= min_delta = 0.0. New best score: 0.213\n",
      "Epoch 16, global step 798: val_loss reached 0.21295 (best 0.21295), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=16_val_loss=0.2130.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 18\n",
      "val_loss = 0.213\n",
      "val_perplexity = 1.2373\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d3579ddee246ecaca582f48a7d9761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.016 >= min_delta = 0.0. New best score: 0.197\n",
      "Epoch 17, global step 845: val_loss reached 0.19664 (best 0.19664), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=17_val_loss=0.1966.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 19\n",
      "val_loss = 0.1966\n",
      "val_perplexity = 1.2173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47848bcf42f941d590779dd40650ae02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.036 >= min_delta = 0.0. New best score: 0.161\n",
      "Epoch 18, global step 892: val_loss reached 0.16104 (best 0.16104), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=18_val_loss=0.1610.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 20\n",
      "val_loss = 0.161\n",
      "val_perplexity = 1.1747\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6022e3ac3b4635be3ca1260e935109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.020 >= min_delta = 0.0. New best score: 0.141\n",
      "Epoch 19, global step 939: val_loss reached 0.14071 (best 0.14071), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=19_val_loss=0.1407.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 21\n",
      "val_loss = 0.1407\n",
      "val_perplexity = 1.1511\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e19763de861459b9c6ea7784f953bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.059 >= min_delta = 0.0. New best score: 0.082\n",
      "Epoch 20, global step 986: val_loss reached 0.08185 (best 0.08185), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=20_val_loss=0.0819.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 22\n",
      "val_loss = 0.0819\n",
      "val_perplexity = 1.0853\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1c0106d67a45c6b39e6fe65e843317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.022 >= min_delta = 0.0. New best score: 0.060\n",
      "Epoch 21, global step 1033: val_loss reached 0.06030 (best 0.06030), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=21_val_loss=0.0603.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 23\n",
      "val_loss = 0.0603\n",
      "val_perplexity = 1.0622\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa202ce7455b457c9d82aa6d4768e954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.020 >= min_delta = 0.0. New best score: 0.040\n",
      "Epoch 22, global step 1080: val_loss reached 0.04049 (best 0.04049), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=22_val_loss=0.0405.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 24\n",
      "val_loss = 0.0405\n",
      "val_perplexity = 1.0413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d05c806363641eaa0c51125893548ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.034\n",
      "Epoch 23, global step 1127: val_loss reached 0.03369 (best 0.03369), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=23_val_loss=0.0337.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 25\n",
      "val_loss = 0.0337\n",
      "val_perplexity = 1.0343\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7e47cb931b43d694a1724cd7438d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.025\n",
      "Epoch 24, global step 1174: val_loss reached 0.02486 (best 0.02486), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=24_val_loss=0.0249.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 26\n",
      "val_loss = 0.0249\n",
      "val_perplexity = 1.0252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea63bf6bbbdf403a9882b037f38496f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.019\n",
      "Epoch 25, global step 1221: val_loss reached 0.01901 (best 0.01901), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=25_val_loss=0.0190.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 27\n",
      "val_loss = 0.019\n",
      "val_perplexity = 1.0192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d72ce299794cc888103cbc4ca15491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.015\n",
      "Epoch 26, global step 1268: val_loss reached 0.01495 (best 0.01495), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=26_val_loss=0.0150.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 28\n",
      "val_loss = 0.015\n",
      "val_perplexity = 1.0151\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959885e6a64c42bebb6511001cd9543e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.012\n",
      "Epoch 27, global step 1315: val_loss reached 0.01184 (best 0.01184), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=27_val_loss=0.0118.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 29\n",
      "val_loss = 0.0118\n",
      "val_perplexity = 1.0119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c846b5819c447389807139dbb2b6033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.010\n",
      "Epoch 28, global step 1362: val_loss reached 0.00965 (best 0.00965), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=28_val_loss=0.0097.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 30\n",
      "val_loss = 0.0097\n",
      "val_perplexity = 1.0097\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1926f64f3f2e428798423db9e686cf47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.008\n",
      "Epoch 29, global step 1409: val_loss reached 0.00830 (best 0.00830), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=29_val_loss=0.0083.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 31\n",
      "val_loss = 0.0083\n",
      "val_perplexity = 1.0083\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc6f83f75104bbbb74746594d88e4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.007\n",
      "Epoch 30, global step 1456: val_loss reached 0.00716 (best 0.00716), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=30_val_loss=0.0072.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 32\n",
      "val_loss = 0.0072\n",
      "val_perplexity = 1.0072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac76507a24e4eee80d77612dd653edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.006\n",
      "Epoch 31, global step 1503: val_loss reached 0.00621 (best 0.00621), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=31_val_loss=0.0062.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 33\n",
      "val_loss = 0.0062\n",
      "val_perplexity = 1.0062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e7a2c7614b4d80aca2020c4c613c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 34\n",
      "val_loss = 0.0056\n",
      "val_perplexity = 1.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32, global step 1550: val_loss reached 0.00558 (best 0.00558), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=32_val_loss=0.0056.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7fb659ab0c4e8d9f2fe5e20af8feb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.005\n",
      "Epoch 33, global step 1597: val_loss reached 0.00504 (best 0.00504), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=33_val_loss=0.0050.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 35\n",
      "val_loss = 0.005\n",
      "val_perplexity = 1.0051\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017368854a3449da80409e152750a738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.005\n",
      "Epoch 34, global step 1644: val_loss reached 0.00467 (best 0.00467), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=34_val_loss=0.0047.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 36\n",
      "val_loss = 0.0047\n",
      "val_perplexity = 1.0047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4f956c04894cdb82d88eef93de9dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.004\n",
      "Epoch 35, global step 1691: val_loss reached 0.00439 (best 0.00439), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=35_val_loss=0.0044.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 37\n",
      "val_loss = 0.0044\n",
      "val_perplexity = 1.0044\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a28d80a157247f0ae451f9ce11bdc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.004\n",
      "Epoch 36, global step 1738: val_loss reached 0.00399 (best 0.00399), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=36_val_loss=0.0040.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 38\n",
      "val_loss = 0.004\n",
      "val_perplexity = 1.004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae599e773a6440e3991e517e30bef791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.004\n",
      "Epoch 37, global step 1785: val_loss reached 0.00371 (best 0.00371), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=37_val_loss=0.0037.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 39\n",
      "val_loss = 0.0037\n",
      "val_perplexity = 1.0037\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637a96f9c30949ba86b00c532df25eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.004\n",
      "Epoch 38, global step 1832: val_loss reached 0.00352 (best 0.00352), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=38_val_loss=0.0035.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 40\n",
      "val_loss = 0.0035\n",
      "val_perplexity = 1.0035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b169f1239240c98d0ba2ccc65279c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.003\n",
      "Epoch 39, global step 1879: val_loss reached 0.00334 (best 0.00334), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=39_val_loss=0.0033.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 41\n",
      "val_loss = 0.0033\n",
      "val_perplexity = 1.0033\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9da01c26bbc4414b9a1576c0f3d0e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.003\n",
      "Epoch 40, global step 1926: val_loss reached 0.00312 (best 0.00312), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=40_val_loss=0.0031.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 42\n",
      "val_loss = 0.0031\n",
      "val_perplexity = 1.0031\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed97680cf5ce4518a71e2a3defdf06b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.003\n",
      "Epoch 41, global step 1973: val_loss reached 0.00296 (best 0.00296), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=41_val_loss=0.0030.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 43\n",
      "val_loss = 0.003\n",
      "val_perplexity = 1.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418270c74d184a8fb7aefe278cc72563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.003\n",
      "Epoch 42, global step 2020: val_loss reached 0.00278 (best 0.00278), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=42_val_loss=0.0028.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 44\n",
      "val_loss = 0.0028\n",
      "val_perplexity = 1.0028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551f638d76984f139f9cd3fd7e459e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.003\n",
      "Epoch 43, global step 2067: val_loss reached 0.00266 (best 0.00266), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=43_val_loss=0.0027.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 45\n",
      "val_loss = 0.0027\n",
      "val_perplexity = 1.0027\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e43d41e1dc4b8a9d4ea3efb2092734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.003\n",
      "Epoch 44, global step 2114: val_loss reached 0.00264 (best 0.00264), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=44_val_loss=0.0026.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 46\n",
      "val_loss = 0.0026\n",
      "val_perplexity = 1.0026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43a2a6a32c04702bf695efe6b1116c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.002\n",
      "Epoch 45, global step 2161: val_loss reached 0.00241 (best 0.00241), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=45_val_loss=0.0024.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 47\n",
      "val_loss = 0.0024\n",
      "val_perplexity = 1.0024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3d46794ad84c6d84621f6bdbca3185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.002\n",
      "Epoch 46, global step 2208: val_loss reached 0.00229 (best 0.00229), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=46_val_loss=0.0023.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 48\n",
      "val_loss = 0.0023\n",
      "val_perplexity = 1.0023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47af36a703294fbe9dd249b107ee85c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.002\n",
      "Epoch 47, global step 2255: val_loss reached 0.00223 (best 0.00223), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=47_val_loss=0.0022.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 49\n",
      "val_loss = 0.0022\n",
      "val_perplexity = 1.0022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a140d5227c094d5dbdc91d34a959c7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.002\n",
      "Epoch 48, global step 2302: val_loss reached 0.00208 (best 0.00208), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=48_val_loss=0.0021.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 50\n",
      "val_loss = 0.0021\n",
      "val_perplexity = 1.0021\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4a3ed7c06a4dffb3273b6a32191ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.002\n",
      "Epoch 49, global step 2349: val_loss reached 0.00200 (best 0.00200), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=49_val_loss=0.0020.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 51\n",
      "val_loss = 0.002\n",
      "val_perplexity = 1.002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4954cfb300834d5f87cd011842e3fb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.002\n",
      "Epoch 50, global step 2396: val_loss reached 0.00195 (best 0.00195), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=50_val_loss=0.0020.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 52\n",
      "val_loss = 0.002\n",
      "val_perplexity = 1.002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45dc8cd3f6b4a55a391686fe729aafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.002\n",
      "Epoch 51, global step 2443: val_loss reached 0.00195 (best 0.00195), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=51_val_loss=0.0019.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 53\n",
      "val_loss = 0.0019\n",
      "val_perplexity = 1.0019\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6363fdcb9e6418385fd6c353a8d4a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.002\n",
      "Epoch 52, global step 2490: val_loss reached 0.00178 (best 0.00178), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=52_val_loss=0.0018.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 54\n",
      "val_loss = 0.0018\n",
      "val_perplexity = 1.0018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604b923738e448dbbd103b742f64c8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.002\n",
      "Epoch 53, global step 2537: val_loss reached 0.00163 (best 0.00163), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=53_val_loss=0.0016.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 55\n",
      "val_loss = 0.0016\n",
      "val_perplexity = 1.0016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa8fca8fe264269bcca9cb5a4315f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54, global step 2584: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 56\n",
      "val_loss = 0.0017\n",
      "val_perplexity = 1.0017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b361e64598b848848d3a9761faacf614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55, global step 2631: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 57\n",
      "val_loss = 0.0016\n",
      "val_perplexity = 1.0016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84568ab65d4438bba4638bb598fc35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.002\n",
      "Epoch 56, global step 2678: val_loss reached 0.00150 (best 0.00150), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=56_val_loss=0.0015.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 58\n",
      "val_loss = 0.0015\n",
      "val_perplexity = 1.0015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f04dc31d7845ee8241dbf6e8a40b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57, global step 2725: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 59\n",
      "val_loss = 0.0015\n",
      "val_perplexity = 1.0015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ff9e7ee68e481b9dd74c46c2889e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.001\n",
      "Epoch 58, global step 2772: val_loss reached 0.00140 (best 0.00140), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=58_val_loss=0.0014.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 60\n",
      "val_loss = 0.0014\n",
      "val_perplexity = 1.0014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdae1187e5245b6a6197334ffc139d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.001\n",
      "Epoch 59, global step 2819: val_loss reached 0.00131 (best 0.00131), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=59_val_loss=0.0013.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 61\n",
      "val_loss = 0.0013\n",
      "val_perplexity = 1.0013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbaf9ea0edaf4963b5b7b32a61024ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60, global step 2866: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 62\n",
      "val_loss = 0.0702\n",
      "val_perplexity = 1.0727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965ef3cc4a334d3f989eb2cd8a3e7254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61, global step 2913: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 63\n",
      "val_loss = 0.0026\n",
      "val_perplexity = 1.0026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45660fa6092b4112b665c7350a39739c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62, global step 2960: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 64\n",
      "val_loss = 0.002\n",
      "val_perplexity = 1.002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255949bf9ab442f09918014693ae0869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63, global step 3007: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 65\n",
      "val_loss = 0.0016\n",
      "val_perplexity = 1.0016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef284f81bf24e14958238356506f33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64, global step 3054: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 66\n",
      "val_loss = 0.0014\n",
      "val_perplexity = 1.0014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5ffd3eec34480585920da195563d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65, global step 3101: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 67\n",
      "val_loss = 0.0013\n",
      "val_perplexity = 1.0013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de4e9e6003f494c969dfd89147605a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.001\n",
      "Epoch 66, global step 3148: val_loss reached 0.00125 (best 0.00125), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=66_val_loss=0.0013.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 68\n",
      "val_loss = 0.0013\n",
      "val_perplexity = 1.0013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cd0de03c3542fc82d0a162114fca67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.001\n",
      "Epoch 67, global step 3195: val_loss reached 0.00113 (best 0.00113), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=67_val_loss=0.0011.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 69\n",
      "val_loss = 0.0011\n",
      "val_perplexity = 1.0011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf21f93bf2a4129bf4511b45fb28b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.001\n",
      "Epoch 68, global step 3242: val_loss reached 0.00113 (best 0.00113), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=68_val_loss=0.0011.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 70\n",
      "val_loss = 0.0011\n",
      "val_perplexity = 1.0011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3940872651104275ad8bd9ff04455166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.001\n",
      "Epoch 69, global step 3289: val_loss reached 0.00112 (best 0.00112), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=69_val_loss=0.0011.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 71\n",
      "val_loss = 0.0011\n",
      "val_perplexity = 1.0011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fda11f2147493db794540c1392d16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.001\n",
      "Epoch 70, global step 3336: val_loss reached 0.00101 (best 0.00101), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=70_val_loss=0.0010.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 72\n",
      "val_loss = 0.001\n",
      "val_perplexity = 1.001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cc513171a841d89a7330b4a29987de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71, global step 3383: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 73\n",
      "val_loss = 0.001\n",
      "val_perplexity = 1.001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c93e8caa5c41b1b6edb86e29ad93ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72, global step 3430: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 74\n",
      "val_loss = 0.0011\n",
      "val_perplexity = 1.0011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b0e236548742a48242bf6c60410370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73, global step 3477: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 75\n",
      "val_loss = 0.0011\n",
      "val_perplexity = 1.0011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6ac8eb32394aa08ac58dbd9e744c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74, global step 3524: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 76\n",
      "val_loss = 0.0011\n",
      "val_perplexity = 1.0011\n",
      "Best metric value for fold0\n",
      "val_loss  = 0.0010137816425412893\n",
      "{'val_loss': (0.001, 71), 'val_perplexity': (1.001, 71)}\n"
     ]
    }
   ],
   "source": [
    "find_lr = True\n",
    "fold_loss = []\n",
    "fold_metrics = []\n",
    "\n",
    "for fold in range(Config.NUM_FOLDS):\n",
    "    dl_train, dl_val, ds_train, ds_val, h_vocab, m_vocab, inv_h_vocab, inv_m_vocab = get_fold_dls(fold, df_dates)\n",
    "    run_training(fold, fold_loss, fold_metrics, dl_train, dl_val, h_vocab, m_vocab, find_lr=False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_from_value(dict, value_to_search):\n",
    "    for key, value in dict.items():\n",
    "        if value == value_to_search:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on test set\n",
    "\n",
    "def run_prediction():\n",
    "    test_dataset = load_date_dataset(num_examples=10)\n",
    "    Config.BATCH_SIZE = 1\n",
    "    df_dates_test = pd.DataFrame({\"h_dt\": test_dataset[:, 0], \"m_dt\": test_dataset[:, 1]})\n",
    "    h_dt_max_len = df_dates_test.h_dt.apply(lambda x: len(x)).max()\n",
    "    transform = StoITensorTransform(h_vocab, h_dt_max_len)\n",
    "    target_transform = StoITensorTransform(m_vocab, len(m_vocab), add_sos_token=True)\n",
    "    ds_test = DateDataset(df_dates_test.h_dt, df_dates_test.m_dt, transform=transform, target_transform=target_transform)    \n",
    "    dl_test = DataLoader(ds_test, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS, collate_fn=pad_collate)\n",
    "    seq2seq_model = EncoderDecoderLitModel.load_from_checkpoint(\n",
    "        checkpoint_path = \"model/best_model_epoch=70_val_loss=0.0010.ckpt\",\n",
    "        hparams = model_params, \n",
    "        source_vocab_size = len(h_vocab),\n",
    "        target_vocab_size = len(m_vocab)\n",
    "    ).to(Config.DEVICE)\n",
    "    pred_table = []\n",
    "    for src_seq, src_seq_len, target_seq in dl_test:        \n",
    "        src_seq = src_seq.to(Config.DEVICE)\n",
    "        src_seq_len = src_seq_len.to(Config.DEVICE)\n",
    "        target_seq = target_seq.to(Config.DEVICE)\n",
    "        target_seq_oh = one_hot_encode(target_seq, len(m_vocab))\n",
    "        outputs = seq2seq_model(src_seq, src_seq_len, target_seq_oh)\n",
    "        pred_target_seq = outputs.argmax(2)[:, 1:].reshape(-1).cpu().tolist()\n",
    "        target_seq = target_seq[:, 1:].reshape(-1).cpu().tolist()                \n",
    "        src_seq = src_seq.reshape(-1).cpu().tolist()\n",
    "        source_str = ''.join([get_key_from_value(h_vocab, index) for index in src_seq])                \n",
    "        target_str = ''.join([inv_m_vocab[index] for index in target_seq])        \n",
    "        pred_target_str = ''.join([get_key_from_value(m_vocab, index) for index in pred_target_seq])        \n",
    "        pred_table.append([source_str, target_str, pred_target_str])        \n",
    "    return pred_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sequence          Target Sequence    Predicted Target Sequence\n",
      "-----------------------  -----------------  ---------------------------\n",
      "27 September 1997        1997-09-27         1997-09-27\n",
      "Friday February 6 1970   1970-02-06         1970-02-06\n",
      "4/27/05                  2005-04-27         2005-04-27\n",
      "02.11.09                 2009-11-02         2009-11-02\n",
      "Thursday June 20 1996    1996-06-20         1996-06-20\n",
      "Friday November 9 2001   2001-11-09         2001-11-09\n",
      "Saturday April 11 2009   2009-04-11         2009-04-11\n",
      "Monday December 27 1982  1982-12-27         1982-12-27\n",
      "17.07.04                 2004-07-17         2004-07-17\n",
      "Thursday May 13 2021     2021-05-13         2021-05-13\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "pred_table = run_prediction()    \n",
    "header = [\"Source sequence\", \"Target Sequence\", \"Predicted Target Sequence\"]\n",
    "print(tabulate(pred_table, headers=header))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0197751694b00855cd01780d565fa2e16f7945f624c4146f8d6aac863c2ba178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('fastai': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
