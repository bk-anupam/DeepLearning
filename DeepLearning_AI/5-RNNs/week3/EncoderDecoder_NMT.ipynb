{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import babel\n",
    "from babel.dates import format_date\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from sklearn import model_selection\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "Faker.seed(12345)\n",
    "random.seed(12345)\n",
    "\n",
    "# Define format of the data we would like to generate\n",
    "FORMATS = ['short',\n",
    "           'medium',\n",
    "           'long',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'd MMM YYY', \n",
    "           'd MMMM YYY',\n",
    "           'dd MMM YYY',\n",
    "           'd MMM, YYY',\n",
    "           'd MMMM, YYY',\n",
    "           'dd, MMM YYY',\n",
    "           'd MM YY',\n",
    "           'd MMMM YYY',\n",
    "           'MMMM d YYY',\n",
    "           'MMMM d, YYY',\n",
    "           'dd.MM.YY']\n",
    "\n",
    "# change this if you want it to work with another language\n",
    "LOCALES = ['en_US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Config:\n",
    "    RANDOM_STATE_SEED = 42\n",
    "    Tx_max = 30\n",
    "    Ty_max = 11\n",
    "    BATCH_SIZE = 128\n",
    "    NUM_WORKERS = 8\n",
    "    NUM_EPOCHS = 25\n",
    "    PRECISION = 16\n",
    "    NUM_FOLDS = 5\n",
    "    FAST_DEV_RUN = False\n",
    "    DEVICE = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_data():\n",
    "    dt = fake.date_object()\n",
    "    human_readable_dt = None\n",
    "    machine_readable_dt = None\n",
    "    try:\n",
    "        human_readable_dt = format_date(dt, random.choice(FORMATS), \"en_US\")\n",
    "        human_readable_dt = human_readable_dt.replace(\",\", \"\")\n",
    "        machine_readable_dt = dt.isoformat()\n",
    "    except AttributeError as e:\n",
    "        return None, None, None\n",
    "    return human_readable_dt, machine_readable_dt, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_date_dataset(num_examples=100):    \n",
    "    dataset = []\n",
    "    for row in range(num_examples):\n",
    "        h_dt, m_dt, dt = generate_date_data()        \n",
    "        dataset.append([h_dt, m_dt])    \n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vocab for both source and target sequences needs to be generated from training data only\n",
    "# to prevent data leakage into the validation sets leading to inflated model accuracy in validation phase\n",
    "def get_source_target_vocab(human_dates, machine_dates):\n",
    "    human_dt_vocab = set()\n",
    "    machine_dt_vocab = set()\n",
    "    for (h_dt, m_dt) in zip(human_dates, machine_dates):\n",
    "        human_dt_vocab.update(tuple(h_dt))\n",
    "        machine_dt_vocab.update(tuple(m_dt))\n",
    "    human_dt_vocab = {value: index for index, value in enumerate(sorted(human_dt_vocab) + ['<unk>', '<pad>', '<sos>', '<eos>'])}\n",
    "    machine_dt_vocab = {value: index for index, value in enumerate(sorted(machine_dt_vocab) + ['<sos>'])}\n",
    "    inv_machine_dt_vocab = dict(enumerate(sorted(machine_dt_vocab)))       \n",
    "    return human_dt_vocab, machine_dt_vocab, inv_machine_dt_vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stoi(str, length, vocab, add_sos_token=False):\n",
    "    \"\"\"\n",
    "    Converts all strings in the vocabulary into a list of integers representing the positions of the\n",
    "    input string's characters in the \"vocab\"\n",
    "    \n",
    "    Arguments:\n",
    "    string -- input string, e.g. 'Wed 10 Jul 2007'\n",
    "    length -- the number of time steps you'd like, determines if the output will be padded or cut\n",
    "    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n",
    "    \n",
    "    Returns:\n",
    "    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n",
    "    \"\"\"\n",
    "    str = str.lower()\n",
    "    str = str.replace(\",\", \"\")\n",
    "    if len(str) > length:\n",
    "        str = str[:length]\n",
    "    unk_index = vocab.get(\"<unk>\")            \n",
    "    char_indexes = [vocab.get(char, unk_index) for char in str]\n",
    "    if add_sos_token:\n",
    "        sos_index = vocab.get(\"<sos>\")        \n",
    "        # We add index corresponding to <sos> token to the start of target date sequence\n",
    "        char_indexes.insert(0, sos_index)\n",
    "    return np.array(char_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training dataframe into kfolds for cross validation. We do this before any processing is done\n",
    "# on the data. We use stratified kfold if the target distribution is unbalanced\n",
    "def strat_kfold_dataframe(df, target_col_name, num_folds=5):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # randomize of shuffle the rows of dataframe before splitting is done\n",
    "    df = df.sample(frac=1, random_state=Config.RANDOM_STATE_SEED).reset_index(drop=True)\n",
    "    # get the target data\n",
    "    y = df[target_col_name].values\n",
    "    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=Config.RANDOM_STATE_SEED)\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n",
    "        df.loc[val_index, \"kfold\"] = fold\n",
    "    return df        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h_dt</th>\n",
       "      <th>m_dt</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>February 3 1993</td>\n",
       "      <td>1993-02-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Friday September 22 1995</td>\n",
       "      <td>1995-09-22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Saturday May 29 1999</td>\n",
       "      <td>1999-05-29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday October 16 2008</td>\n",
       "      <td>2008-10-16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/15/02</td>\n",
       "      <td>2002-06-15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       h_dt        m_dt  kfold\n",
       "0           February 3 1993  1993-02-03      0\n",
       "1  Friday September 22 1995  1995-09-22      2\n",
       "2      Saturday May 29 1999  1999-05-29      3\n",
       "3  Thursday October 16 2008  2008-10-16      1\n",
       "4                   6/15/02  2002-06-15      2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_date_dataset(10000)\n",
    "# Let us create a dates dataframe that will contain training data of human readable and machine\n",
    "# readable dates\n",
    "df_dates = pd.DataFrame({\"h_dt\": dataset[:, 0], \"m_dt\": dataset[:, 1]})\n",
    "df_dates = strat_kfold_dataframe(df_dates, target_col_name=\"m_dt\", num_folds=5)  \n",
    "df_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_vocab, m_vocab, inv_m_vocab = get_source_target_vocab(df_dates.h_dt, df_dates.m_dt)\n",
    "# h_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = stoi(df_dates.h_dt[0], 30, h_vocab)\n",
    "# torch.from_numpy(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converts a vectorized date ( which is a tensor of ints where each int is the \n",
    "# # position of the corresponding char in the relevant vocab ) to one hot encoded form\n",
    "# class OneHotTransform(object):\n",
    "#     def __init__(self, vocab, max_seq_length):\n",
    "#         self.vocab = vocab\n",
    "#         self.max_seq_length = max_seq_length\n",
    "\n",
    "#     def __call__(self, X):\n",
    "#         pad_index = self.vocab[\"<pad>\"]\n",
    "#         pad_sequence()\n",
    "#         return F.one_hot(X.T.long(), len(self.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts a string (sequence of chars) to a tensor of ints where each int is the \n",
    "# position of the corresponding char in the relevant vocab\n",
    "class StoITensorTransform(object):\n",
    "    def __init__(self, vocab, max_seq_length, add_sos_token=False):\n",
    "        self.vocab = vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.add_sos_token = add_sos_token\n",
    "\n",
    "    def __call__(self, X):\n",
    "        vectorized_str = stoi(X, self.max_seq_length, self.vocab, self.add_sos_token)\n",
    "        #print(vectorized_str)\n",
    "        return torch.from_numpy(vectorized_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch is the return value of __getitem__ method of the dataset being used. For DateDataset it is h_dt, m_dt\n",
    "def pad_collate(batch):\n",
    "    # we want to pad the h_dt sequences as these can be of variable length.\n",
    "    # h_dt is of shape len(h_dt)\n",
    "    sorted_batch = sorted(batch, key=lambda x:x[0].shape[0], reverse=True)\n",
    "    h_dt_sorted = [x[0] for x in sorted_batch]\n",
    "    h_dt_padded = pad_sequence(h_dt_sorted, batch_first = True, padding_value=0)\n",
    "    # the original length of the padded h_dt sequences\n",
    "    h_dt_len = torch.Tensor([len(x) for x in h_dt_sorted])\n",
    "    # unpadded m_dt sequences    \n",
    "    m_dt = torch.stack([x[1] for x in sorted_batch])        \n",
    "    return h_dt_padded, h_dt_len, m_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateDataset(Dataset):\n",
    "    def __init__(self, human_fmt_dates, machine_fmt_dates, transform, target_transform):\n",
    "        super().__init__()\n",
    "        self.h_dts = human_fmt_dates\n",
    "        self.m_dts = machine_fmt_dates\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform        \n",
    "\n",
    "    # Returns vectorized form of human format date and its corresponding machine format date\n",
    "    # with elements of the vectorized date being the index of the characters in the corresponding date vocab\n",
    "    def __getitem__(self, index):\n",
    "        h_dt = self.h_dts[index]\n",
    "        m_dt = self.m_dts[index]\n",
    "        if self.transform:\n",
    "            h_dt = self.transform(h_dt)\n",
    "        if self.target_transform:\n",
    "            m_dt = self.target_transform(m_dt)\n",
    "        return h_dt, m_dt\n",
    "\n",
    "    def __len__(self):                \n",
    "        return len(self.h_dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train and validation data loaders for a specific fold. \n",
    "# X: numpy array of input features\n",
    "# y: numpy array of target labels\n",
    "# fold: fold index for which to create data loaders                                     \n",
    "# kfolds: Array that marks each of the data items as belonging to a specific fold\n",
    "def get_fold_dls(fold, df):\n",
    "    fold += 1                         \n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    val_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "    h_dt_max_len = train_df.h_dt.apply(lambda x: len(x)).max()\n",
    "    h_vocab, m_vocab, inv_m_vocab = get_source_target_vocab(train_df.h_dt, train_df.m_dt)    \n",
    "    # transform to convert human_date and machine_date to one hot encoded forms\n",
    "    transform = StoITensorTransform(h_vocab, h_dt_max_len)\n",
    "    target_transform = StoITensorTransform(m_vocab, len(m_vocab), add_sos_token=True)\n",
    "    ds_train = DateDataset(train_df.h_dt, train_df.m_dt, transform=transform, target_transform=target_transform)\n",
    "    ds_val = DateDataset(val_df.h_dt, val_df.m_dt, transform=transform, target_transform=target_transform)\n",
    "    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, \n",
    "                        num_workers=Config.NUM_WORKERS, collate_fn=pad_collate)\n",
    "    dl_val = DataLoader(ds_val, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS, \n",
    "                        collate_fn=pad_collate)\n",
    "    return dl_train, dl_val, ds_train, ds_val, h_vocab, m_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train, dl_val, ds_train, ds_val, h_vocab, m_vocab = get_fold_dls(0, df_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11,  2, 10, 10, 10,  0,  1,  6,  0,  3, 10])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tensor = ds_train[2][1]\n",
    "# test_tensor = test_tensor.reshape(len(test_tensor), 1)\n",
    "# print(test_tensor.shape)\n",
    "# m_vocab_size = len(m_vocab)\n",
    "# zeros_tensor = torch.zeros(len(test_tensor), m_vocab_size)\n",
    "# print(zeros_tensor.shape)\n",
    "# src_tensor = torch.ones((len(test_tensor), 1))\n",
    "# print(src_tensor.shape)\n",
    "# m_dt_oh = zeros_tensor.scatter_(1, test_tensor, 1)\n",
    "# print(test_tensor)\n",
    "# print(m_dt_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/what-does-the-scatter-function-do-in-layman-terms/28037/3\n",
    "def one_hot_encode(input, vocab_size):        \n",
    "    #print(f\"input.shape = {input.shape}, vocab_size = {vocab_size}\")\n",
    "    batch_size = input.shape[0]\n",
    "    seq_length = input.shape[1]\n",
    "    input = input.reshape(batch_size, seq_length, 1)    \n",
    "    zeros_tensor = torch.zeros(batch_size, seq_length, vocab_size).to(Config.DEVICE)    \n",
    "    return zeros_tensor.scatter_(2, input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl_train_iter = iter(dl_train)\n",
    "# h_dt, h_dt_len, m_dt = next(dl_train_iter)\n",
    "# test = one_hot_encode(m_dt, len(m_vocab))\n",
    "# test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, drop_out, is_bidirect=False):\n",
    "        super().__init__()\n",
    "        # input to lstm is a sequence (of words, of chars, of anything else). The dimensions being \n",
    "        # (batch_size, sequence_length, input_size) if batch_first = True with sequence_length = length of longest sequence in the batch\n",
    "        # where input_size = number of features(cols) in input X. If you use embedding layer, then each word in the\n",
    "        # the sequence is represented by an embedding vector, so input_size = size of the embedding vector. If one\n",
    "        # hot encoding representation is used then input_size = vocab_size with each word represented by a one hot\n",
    "        # vector with size = vocab_size        \n",
    "        self.input_size = input_size\n",
    "        # hidden_size = number of units in the hidden layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.is_bidirect = is_bidirect\n",
    "        self.num_directions = 2 if is_bidirect else 1\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size = input_size, \n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            dropout = drop_out,\n",
    "            bidirectional = is_bidirect\n",
    "            )                \n",
    "\n",
    "    def forward(self, inputs, input_lengths):       \n",
    "        # inputs = [batch_size, max_seq_length] \n",
    "        # we are going to use one hot encoding representation of the human dates data. The input data is\n",
    "        # vectorized form of human format date with elements of the vectorized date being the index of the \n",
    "        # characters in the corresponding date vocab (input_size = vocab_size)\n",
    "        # inputs_oh = F.one_hot(inputs.T.float(), self.input_size)\n",
    "        #print(f\"inputs.shape = {inputs.shape}\")\n",
    "        inputs_oh = one_hot_encode(inputs, self.input_size)\n",
    "        #print(f\"inputs_oh.shape = {inputs_oh.shape}\")\n",
    "        # inputs_oh = [batch_size, max_seq_length, vocab_size]\n",
    "        # pack_padded_sequence before feeding into LSTM. This is required so pytorch knows\n",
    "        # which elements of the sequence are padded ones and ignore them in computation.\n",
    "        packed_padded_inputs = pack_padded_sequence(inputs_oh, input_lengths.to(\"cpu\"), batch_first=True)\n",
    "        lstm_out_pack, (h_final, c_final) = self.lstm_layer(packed_padded_inputs)\n",
    "        # h_final and c_final = [num_direction * num_layers, batch_size, hidden_size]\n",
    "        # unpack the output\n",
    "        # lstm_out, lstm_out_len = pad_packed_sequence(lstm_out_pack, batch_first=True)\n",
    "        # lstm_out = [batch_size , seq_length , num_directions * hidden_size]\n",
    "        return h_final, c_final\n",
    "\n",
    "    # `nn.LSTM` takes a tuple of hidden states (h0, c0). h0 = initial\n",
    "    # hidden state for each element in the batch, c0 = initial cell state for each element in the batch\n",
    "    def init_state(self, batch_size):\n",
    "        return (\n",
    "            torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size)),\n",
    "            torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, drop_out):\n",
    "        super().__init__()        \n",
    "        self.input_size = input_size        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers                \n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size = input_size, \n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            dropout = drop_out,\n",
    "            bidirectional = False\n",
    "            )   \n",
    "        self.linear = nn.Linear(hidden_size, input_size)                        \n",
    "\n",
    "    def forward(self, input_oh, encoder_hidden, encoder_cell):\n",
    "        # input_oh = [batch_size, target_vocab_size]\n",
    "        # The input sequence length in decoder is always 1 as we feed in one character at a time\n",
    "        input_oh = input_oh.unsqueeze(1).to(Config.DEVICE)\n",
    "        # input_oh = [batch_size, 1, target_vocab_size]        \n",
    "        # inputs_oh = F.one_hot(inputs.T.long(), self.input_size)        \n",
    "        #print(f\"decoder inputs_oh.shape = {inputs_oh.shape}\")\n",
    "        lstm_out, (h_final, c_final) = self.lstm_layer(input_oh, (encoder_hidden, encoder_cell))\n",
    "        #print(f\"decoder lstm_out.shape = {lstm_out.shape}\")\n",
    "        # lstm_out = [batch_size , seq_length , num_directions * hidden_size]\n",
    "        # h_final and c_final = [num_direction * num_layers, batch_size, hidden_size]\n",
    "        # seq_length and num_direction will always be 1 for decoder. Thus\n",
    "        # lstm_out = [batch_size, 1, hidden_size]\n",
    "        # h_final and c_final = [num_layers, batch_size, hidden_size]\n",
    "        pred = self.linear(lstm_out.squeeze(1))\n",
    "        #print(f\"decoder pred.shape = {pred.shape}\")\n",
    "        # pred = [batch_size, output_dim] where output_dim = vocab_size of target sequences ( machine dates in our case)\n",
    "        # For multi class classification the number of output nodes is equal to the number of classes to predict (vocab size)\n",
    "        return pred\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input sequence is of max 30 characters (Tx = 30), a date string like \"January 3, 1984\". The length of each input sequence may vary.\n",
    "Output sequence is of fixed length (Ty=11), a date string like \"1984-01-03\" (10 for digits and one for - character).\n",
    "Vectorized input data is a matrix of dimension batch_size * sequence_length(Tx) * human_date_vocab_size.\n",
    "Take the example of input_date = \"January 3, 1984\". Consider human_date_vocab has 44 characters with each character being mapped to an index. Thus one hot encoded form of an input date character would be vector of 44 zeros with a 1 at the index corresponding to that character.\n",
    "Now our input date is going to be padded to a sequence length of Tx=30. And each of these 30 characters is a vector of length human_date_vocab_size. Similarly output data is matrix of dimension batch_size * sequence_length(Ty) * machine_date_vocab_size.\n",
    "An output_date like \"1984-01-03\" in its one hot encoded for would be a matrix of dimension sequence_length(Ty=11) * machine_date_vocab_size.\n",
    "This holds true for both predicted output dates as well as actual output dates. <br/>\n",
    "\n",
    "Prediction is done for each character in the output date. Thus for a single data row, the model outputs 11 character sequence as output. Each character prediction is a multi class classification problem with loss function (for pytorch) being cross entropy. The sum of losses for each of the 11 character predictions can be considered the loss for a single data row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderLitModel(pl.LightningModule):\n",
    "    def __init__(self, hparams, source_vocab_size, target_vocab_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = hparams[\"lr\"]\n",
    "        self.weight_decay = hparams[\"weight_decay\"]\n",
    "        # target_vocab_size = vocab_size for target sequence data (machine_date) as we are using one hot encoding\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.encoder = lstm_encoder(\n",
    "            input_size = source_vocab_size,\n",
    "            hidden_size = hparams[\"hidden_size\"],\n",
    "            num_layers = hparams[\"num_layers\"],\n",
    "            drop_out = hparams[\"enc_drop_out\"],\n",
    "            is_bidirect = hparams[\"enc_is_bidirect\"]\n",
    "            )\n",
    "        self.decoder = lstm_decoder(\n",
    "            input_size = target_vocab_size,\n",
    "            hidden_size = hparams[\"hidden_size\"],\n",
    "            num_layers = hparams[\"num_layers\"],\n",
    "            drop_out = hparams[\"dec_drop_out\"]\n",
    "        )            \n",
    "\n",
    "    def forward(self, src_seq, src_seq_lengths, target_seq_oh, teacher_forcing_ratio=0.5):\n",
    "        #print(f\"enc_inputs.shape = {enc_inputs.shape}\")\n",
    "        #print(f\"dec_inputs.shape = {dec_inputs.shape}\")\n",
    "        # enc_inputs = [batch_size, max_source_seq_length]\n",
    "        # target_seq = [batch_size, target_seq_length]\n",
    "        batch_size = target_seq_oh.shape[0]        \n",
    "        # target sequence length is 11 as it includes the <sos> token at the begining\n",
    "        target_seq_length = target_seq_oh.shape[1]            \n",
    "        # tensor to store decoder output\n",
    "        dec_outputs = torch.zeros((batch_size, target_seq_length, self.target_vocab_size))        \n",
    "        #print(f\"dec_outputs.shape = {dec_outputs.shape}\")\n",
    "        # first input to the decoder is the <sos> token which is the first character in all target sequences\n",
    "        input = target_seq_oh[:, 0, :].reshape(batch_size, -1)\n",
    "        # input = [batch_size, target_vocab_size]\n",
    "        #print(f\"decoder input.shape = {input.shape}\")\n",
    "        # last hidden and cell state of the encoder is used as initial hidden and cell state of the decoder\n",
    "        enc_hidden, enc_cell = self.encoder(src_seq, src_seq_lengths)\n",
    "        for t in range(1, target_seq_length):            \n",
    "            dec_output = self.decoder(input, enc_hidden, enc_cell)\n",
    "            # dec_output = [batch_size, target_vocab_size]            \n",
    "            dec_outputs[:, t, :] = dec_output\n",
    "            # select the max value out of dec_input_size values for each row\n",
    "            # pred_t = dec_output.argmax(1)\n",
    "            #print(f\"decoder pred_t.shape = {pred_t.shape}\")\n",
    "            # whether to use teacher forcing\n",
    "            teacher_forcing = False if np.random.random() < teacher_forcing_ratio else True            \n",
    "            # if teacher forcing use actual token at t as the input to t+1, otherwise use the prediction at t\n",
    "            # as the input to t+1\n",
    "            actual_t = target_seq_oh[:, t, :]\n",
    "            #print(f\"decoder actual_t.shape = {actual_t.shape}\")\n",
    "            input = actual_t if teacher_forcing else dec_outputs[:, t, :]\n",
    "            input = input.reshape(batch_size, -1)\n",
    "        return dec_outputs            \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, mode=\"min\")\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # data loader batch doesn't perform one hot encoding of either source or target sequences\n",
    "        src_padded_seq, src_seq_lengths, target_seq = batch\n",
    "        # target_seq = [batch_size, target_seq_length]\n",
    "        # src_padded_seq = [batch_size, max_src_seq_length]\n",
    "        target_seq_oh = one_hot_encode(target_seq, self.target_vocab_size)\n",
    "        # target_seq_oh = [batch_size, target_seq_length, target_vocab_size]\n",
    "        pred_target_seq = self(src_padded_seq, src_seq_lengths, target_seq_oh)\n",
    "        # pred_target_seq = [batch_size, target_seq_length, target_vocab_size]\n",
    "        # we will exclude the first character from both the predicted and actual target dates. The first character\n",
    "        # in target_dates in <sos> token while the first value in pred_target_dates is 0.         \n",
    "        #print(f\"target_seq.shape = {target_seq.shape}\")\n",
    "        #print(f\"pred_target_seq.shape = {pred_target_seq.shape}\")\n",
    "        target_seq = target_seq[:, 1:]\n",
    "        #target_seq = target_seq.to(torch.float32)\n",
    "        # flatten the target_seq to 1d \n",
    "        target_seq = target_seq.reshape(-1)\n",
    "        pred_target_seq = pred_target_seq[:, 1:, :].to(Config.DEVICE)\n",
    "        # flatten the predicted target seq to 2d\n",
    "        pred_target_seq = pred_target_seq.view(-1, self.target_vocab_size)\n",
    "        #print(f\"target_seq.shape = {target_seq.shape}\")\n",
    "        #print(f\"pred_target_seq.shape = {pred_target_seq.shape}\")\n",
    "        #print(f\"target sequence row 0:\")\n",
    "        #print(target_seq[0])\n",
    "        #print(f\"pred target sequence row 0:\")\n",
    "        #print(pred_target_seq[0])\n",
    "        loss = cross_entropy(pred_target_seq, target_seq)\n",
    "        #acc = accuracy(pred_target_seq, target_seq)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "        #self.log(\"train_acc\", acc, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):        \n",
    "        src_padded_seq, src_seq_lengths, target_seq = batch\n",
    "        target_seq_oh = one_hot_encode(target_seq, self.target_vocab_size)\n",
    "        # Remember to turn teacher forcing off for validation\n",
    "        pred_target_seq = self(src_padded_seq, src_seq_lengths, target_seq_oh, teacher_forcing_ratio=0)\n",
    "        target_seq = target_seq[:, 1:].reshape(-1)\n",
    "        pred_target_seq = pred_target_seq[:, 1:, :].to(Config.DEVICE)\n",
    "        # flatten the predicted target seq to 2d\n",
    "        pred_target_seq = pred_target_seq.view(-1, self.target_vocab_size)\n",
    "        val_loss = cross_entropy(pred_target_seq, target_seq)\n",
    "        #val_acc = accuracy(pred_target_seq, target_seq)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "        #self.log(\"val_acc\", val_acc, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# For results reproducibility \n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "pl.seed_everything(42, workers=True)\n",
    "# model hyperparameters\n",
    "model_params = {    \n",
    "    \"num_layers\": 2,\n",
    "    \"enc_is_bidirect\": False,\n",
    "    \"hidden_size\": 100,\n",
    "    \"enc_drop_out\": 0.25,\n",
    "    \"dec_drop_out\": 0.25,\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.001\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "\n",
    "class MetricsAggCallback(Callback):\n",
    "    def __init__(self, metric_to_monitor, mode):\n",
    "        self.metric_to_monitor = metric_to_monitor\n",
    "        self.metrics = []\n",
    "        self.best_metric = None\n",
    "        self.mode = mode\n",
    "        self.best_metric_epoch = None\n",
    "\n",
    "    def on_epoch_end(self, trainer: Trainer, pl_module: LightningModule):\n",
    "        metric_value = trainer.callback_metrics[self.metric_to_monitor].cpu().detach().item()\n",
    "        print(f\"metric {self.metric_to_monitor} = {metric_value}\")\n",
    "        self.metrics.append(metric_value)\n",
    "        if self.mode == \"max\":\n",
    "            self.best_metric = max(self.metrics)            \n",
    "        elif self.mode == \"min\":            \n",
    "            self.best_metric = min(self.metrics)           \n",
    "        self.best_metric_epoch = self.metrics.index(self.best_metric)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, fold_loss, dl_train, dl_val, h_vocab, m_vocab, find_lr=True):\n",
    "    fold_str = f\"fold{fold}\"\n",
    "    print(f\"Running training for {fold_str}\")\n",
    "    seq2seq_model = EncoderDecoderLitModel(\n",
    "        hparams = model_params, \n",
    "        source_vocab_size = len(h_vocab),\n",
    "        target_vocab_size = len(m_vocab)\n",
    "        )\n",
    "    tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\")    \n",
    "    chkpt_file_name = \"best_model_{epoch}_{val_loss:.4f}\"\n",
    "    loss_chkpt_callback = ModelCheckpoint(dirpath=\"./model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)    \n",
    "    chkpt_callback = MetricsAggCallback(metric_to_monitor=\"val_loss\", mode=\"min\")\n",
    "    trainer = pl.Trainer(\n",
    "        gpus = 1,\n",
    "        deterministic = True,\n",
    "        auto_select_gpus = True,\n",
    "        progress_bar_refresh_rate = 20,\n",
    "        max_epochs = Config.NUM_EPOCHS,\n",
    "        logger = tb_logger,\n",
    "        auto_lr_find = True,    \n",
    "        precision = Config.PRECISION,   \n",
    "        fast_dev_run = Config.FAST_DEV_RUN, \n",
    "        callbacks = [loss_chkpt_callback, chkpt_callback]\n",
    "    )        \n",
    "    if find_lr:\n",
    "        trainer.tune(model=seq2seq_model, train_dataloaders=dl_train)\n",
    "        print(seq2seq_model.lr)\n",
    "    trainer.fit(seq2seq_model, train_dataloaders=dl_train, val_dataloaders=dl_val)\n",
    "    fold_loss.append(loss_chkpt_callback.best_model_score.cpu().detach().item())\n",
    "    #fold_acc.append(acc_chkpt_callback.best_metric)\n",
    "    print(f\"Loss for {fold_str} = {fold_loss[fold]}\")\n",
    "    del trainer, seq2seq_model, loss_chkpt_callback \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for fold0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:101: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop\n",
      "  rank_zero_warn(f\"you defined a {step_name} but have no {loader_name}. Skipping {stage} loop\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type         | Params\n",
      "-----------------------------------------\n",
      "0 | encoder | lstm_encoder | 140 K \n",
      "1 | decoder | lstm_decoder | 127 K \n",
      "-----------------------------------------\n",
      "267 K     Trainable params\n",
      "0         Non-trainable params\n",
      "267 K     Total params\n",
      "1.070     Total estimated model params size (MB)\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4db4d4c8963424ababb9511c2f1fd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint file at /home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/lr_find_temp_model.ckpt\n",
      "Restored all states from the checkpoint file at /home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/lr_find_temp_model.ckpt\n",
      "Learning rate set to 0.005754399373371567\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type         | Params\n",
      "-----------------------------------------\n",
      "0 | encoder | lstm_encoder | 140 K \n",
      "1 | decoder | lstm_decoder | 127 K \n",
      "-----------------------------------------\n",
      "267 K     Trainable params\n",
      "0         Non-trainable params\n",
      "267 K     Total params\n",
      "1.070     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005754399373371567\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8599cc6df04782ad0f1f180579c5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 2.492827892303467\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b0423a41104dfa93f80a810e7d5a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 36it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71da7d1c7584c0cba1ed5f14d532c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 62: val_loss reached 1.84141 (best 1.84141), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=0_val_loss=1.8414.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.8414080142974854\n",
      "metric val_loss = 1.8414080142974854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe2e70f999640eb91ec6815f04c36a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 125: val_loss reached 1.62704 (best 1.62704), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=1_val_loss=1.6270.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.6270416975021362\n",
      "metric val_loss = 1.6270416975021362\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e1f78feaa143498c6b8c038fafc5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 188: val_loss reached 1.49631 (best 1.49631), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=2_val_loss=1.4963.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.4963139295578003\n",
      "metric val_loss = 1.4963139295578003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d599017b922c402190fcd41536745138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 251: val_loss reached 1.46106 (best 1.46106), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=3_val_loss=1.4611.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.4610594511032104\n",
      "metric val_loss = 1.4610594511032104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa058f3e823643bf8191c5b3be58153f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 314: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.474239706993103\n",
      "metric val_loss = 1.474239706993103\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e110a581b16d495bbcd0eaae9af5319c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 377: val_loss reached 1.44059 (best 1.44059), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=5_val_loss=1.4406.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.4405889511108398\n",
      "metric val_loss = 1.4405889511108398\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fdc925be2f4ab88af25991b352163f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 440: val_loss reached 1.37669 (best 1.37669), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=6_val_loss=1.3767.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.376690149307251\n",
      "metric val_loss = 1.376690149307251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f92881607c471b928e062791172ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 503: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.4169812202453613\n",
      "metric val_loss = 1.4169812202453613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cc97e5dfd540d58a378741566c491e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 566: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.4245829582214355\n",
      "metric val_loss = 1.4245829582214355\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f4b991d0ce44ce834e82f840bf6056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 629: val_loss reached 1.33121 (best 1.33121), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=9_val_loss=1.3312.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.3312138319015503\n",
      "metric val_loss = 1.3312138319015503\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83819f7f460450e91f3ce58d951da7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 692: val_loss reached 1.32001 (best 1.32001), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=10_val_loss=1.3200.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.3200061321258545\n",
      "metric val_loss = 1.3200061321258545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fea723bf3646119b38682c2fa9ad9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 755: val_loss reached 1.27357 (best 1.27357), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=11_val_loss=1.2736.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.2735685110092163\n",
      "metric val_loss = 1.2735685110092163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358c6e6ca3054ab591f758acbe636e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 818: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.2836506366729736\n",
      "metric val_loss = 1.2836506366729736\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e520b3f7b5834cf5a761dd711034940a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 881: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.288377046585083\n",
      "metric val_loss = 1.288377046585083\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28a0453e4da4125ab26892a92a88bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 944: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.3186700344085693\n",
      "metric val_loss = 1.3186700344085693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378c35ae731441c79259ff7361a763cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 1007: val_loss reached 1.22411 (best 1.22411), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=15_val_loss=1.2241.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.224107027053833\n",
      "metric val_loss = 1.224107027053833\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffafecebbad471ea6978570ed299ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 1070: val_loss reached 1.22373 (best 1.22373), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=16_val_loss=1.2237.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.223727822303772\n",
      "metric val_loss = 1.223727822303772\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787caafdb5514e15921ecfd2ce7bbb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 1133: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.2516676187515259\n",
      "metric val_loss = 1.2516676187515259\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fda93fcec949c9ad249cfdbc738a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 1196: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.2245814800262451\n",
      "metric val_loss = 1.2245814800262451\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b07138e886473994278e5debfa8d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 1259: val_loss reached 1.19415 (best 1.19415), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=19_val_loss=1.1941.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.1941465139389038\n",
      "metric val_loss = 1.1941465139389038\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69236a6c6ac54e6aac0b81919b0576ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 1322: val_loss reached 1.18230 (best 1.18230), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=20_val_loss=1.1823.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.1822988986968994\n",
      "metric val_loss = 1.1822988986968994\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dac865b14bb43f7b71b515ddfae284c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 1385: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.2507240772247314\n",
      "metric val_loss = 1.2507240772247314\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec753484fb144828a07b8ac7668d76ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 1448: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.1969553232192993\n",
      "metric val_loss = 1.1969553232192993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2c3165b1b74e56bfd73a7845934ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 1511: val_loss reached 1.15944 (best 1.15944), saving model to \"/home/bk_anupam/code/ML/DeepLearning/DeepLearning_AI/5-RNNs/week3/model/best_model_epoch=23_val_loss=1.1594.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_loss = 1.1594374179840088\n",
      "metric val_loss = 1.1594374179840088\n"
     ]
    }
   ],
   "source": [
    "find_lr = True\n",
    "fold_loss = []\n",
    "\n",
    "for fold in range(Config.NUM_FOLDS):\n",
    "    dl_train, dl_val, ds_train, ds_val, h_vocab, m_vocab = get_fold_dls(fold, df_dates)\n",
    "    run_training(fold, fold_loss, dl_train, dl_val, h_vocab, m_vocab)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(input)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0197751694b00855cd01780d565fa2e16f7945f624c4146f8d6aac863c2ba178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('fastai': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
