{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration constants\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1, padding=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, \n",
    "                    padding=padding, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        # The first conv layer downsamples the input if stride > 1. This happens at the begining\n",
    "        # of the 2nd, 3rd and 4th residual blocks. \n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # If the input has been downsampled by conv1 operation, correspondingly the input needs \n",
    "        # to be downsampled by the downsample operation before it can be added to the output of \n",
    "        # the residual block\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block : Type of block (BasicBlock or Bottleneck block)\n",
    "def _make_layer(block, in_channels, out_channels, blocks, stride=1):\n",
    "    downsample = None\n",
    "    # if the conv layer performs a downsample of input, we need a downsample function to perform an equivalent\n",
    "    # scaling of x.\n",
    "    if in_channels != out_channels or stride > 1:\n",
    "        downsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "    layers = []\n",
    "    # The first conv layer in the block that performs downsampling of the input\n",
    "    layers.append(block(in_channels, out_channels, stride, downsample))     \n",
    "    # the remaning layers in the block do no downsampling\n",
    "    in_channels = out_channels    \n",
    "    for _ in range(1, blocks):\n",
    "        layers.append(block(in_channels, out_channels))\n",
    "    return nn.Sequential(*layers)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torchmetrics.functional import accuracy\n",
    "import torchmetrics\n",
    "\n",
    "# The input image size is 224 * 224 * 3\n",
    "class ResNetLit(pl.LightningModule):\n",
    "    def __init__(self, block, layers, num_classes, lr, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.lr = lr\n",
    "        self.val_metric = torchmetrics.Accuracy()\n",
    "        self.layers = layers\n",
    "        self.num_classes = num_classes          \n",
    "        self.conv1_out_channels = 64     \n",
    "        # in=224, out=112\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=self.conv1_out_channels, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1_out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # in = 112, out = 56\n",
    "        # out = ((in - padding) / stride) + 1\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # The first layer with 3 residual blocks stacked. No downsampling happens in the first layer\n",
    "        # in = 56, out = 56, in_channels = out_channels = 64\n",
    "        self.layer1 = _make_layer(block, in_channels=self.conv1_out_channels, \n",
    "                                    out_channels=self.conv1_out_channels, blocks=layers[0])\n",
    "        # The second layer with 4 residual blocks. \n",
    "        # The first residual block dows downsampling directly by convolutional layers that have a stride of 2\n",
    "        # in = 56, out = 28, in_channels = 64, out_channels = 128\n",
    "        self.layer2 = _make_layer(block, in_channels=self.conv1_out_channels,\n",
    "                                    out_channels=self.conv1_out_channels*2, blocks=layers[1], stride=2)                                    \n",
    "        # The third layer with 6 residual blocks\n",
    "        # in = 28, out = 14, in_channels = 128, out_channels = 256 \n",
    "        self.layer3 = _make_layer(block, in_channels=self.conv1_out_channels*2,\n",
    "                                    out_channels=self.conv1_out_channels*4, blocks=layers[2], stride=2)\n",
    "        # The fourth layer with 3 residual blocks                                    \n",
    "        # in = 14, out = 7, in_channels = 256, out_channels = 512 \n",
    "        self.layer4 = _make_layer(block, in_channels=self.conv1_out_channels*4,\n",
    "                                    out_channels=self.conv1_out_channels*8, blocks=layers[3], stride=2)\n",
    "        # in = 7, out = 1                                       \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(in_features = self.conv1_out_channels*8, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        x = self.conv1(x)   # in: 224, out: 112\n",
    "        x = self.bn1(x)     \n",
    "        x = self.relu(x)    \n",
    "        x = self.maxpool(x) # in: 112, out: 56\n",
    "\n",
    "        x = self.layer1(x)  # in: 56, out: 56\n",
    "        x = self.layer2(x)  # in: 56, out: 28\n",
    "        x = self.layer3(x)  # in: 28, out: 14\n",
    "        x = self.layer4(x)  # in: 14, out: 7\n",
    "\n",
    "        x = self.avgpool(x) # in: 7, out: 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)  \n",
    "        return x    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)        \n",
    "        loss = cross_entropy(y_pred, y)\n",
    "        acc = accuracy(y_pred, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"train_accuracy\", acc, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        val_loss = cross_entropy(y_pred, y)\n",
    "        val_acc = accuracy(y_pred, y)\n",
    "        self.log(\"val_loss\", val_loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"val_accuracy\", val_acc, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, \"min\")        \n",
    "        return {\n",
    "            \"optimizer\": model_optimizer, \n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "data_path = \"./mnist_data\"\n",
    "data_transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                     transforms.Normalize([0.1307], [0.3081])])\n",
    "train_mnist = datasets.MNIST(data_path, train=True, download=True, transform=data_transform)\n",
    "val_mnist = datasets.MNIST(data_path, train=False, download=True, transform=data_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_mnist, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = torch.utils.data.DataLoader(val_mnist, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img, label = train_mnist[0]\n",
    "print(type(img))\n",
    "plt.imshow(img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "mnist_model_lit = ResNetLit(ResidualBlock, layers=[3,4,6,3], num_classes=NUM_CLASSES, lr=0.02, in_channels=1)\n",
    "tb_logger = pl.loggers.TensorBoardLogger(\"logs\", \"mnist_model\")\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"./model\", monitor=\"val_loss\", \n",
    "                                      filename=\"resnet34-{epoch:02d}-{val-loss:.2f}\")\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    auto_select_gpus=True,\n",
    "    progress_bar_refresh_rate=20,\n",
    "    max_epochs=10,\n",
    "    logger=tb_logger,\n",
    "    auto_lr_find=True,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "trainer.tune(model=mnist_model_lit, train_dataloaders=train_loader)\n",
    "mnist_model_lit.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(mnist_model_lit, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNetLit.load_from_checkpoint(\n",
    "#     \"./model/resnet34-epoch=09-val-loss=0.00.ckpt\", \n",
    "#     block=ResidualBlock,\n",
    "#     layers=[3,4,6,3],\n",
    "#     num_classes=10,\n",
    "#     in_channels=1)\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# incorrect = 0\n",
    "# total = 0\n",
    "# predicted_labels_incorrect = []\n",
    "# labels_incorrect = []\n",
    "# with torch.no_grad():\n",
    "#     counter=0\n",
    "#     for imgs, labels in val_loader:                \n",
    "#         predicted_labels = torch.argmax(model(imgs), dim=1)\n",
    "#         total += labels.shape[0]\n",
    "#         correct_pred = predicted_labels == labels\n",
    "#         incorrect_pred = ~correct_pred\n",
    "#         num_incorrect_pred = incorrect_pred.sum()\n",
    "#         incorrect += int(num_incorrect_pred)\n",
    "#         if num_incorrect_pred > 0:\n",
    "#             predicted_labels_incorrect.append(predicted_labels[incorrect_pred].numpy())\n",
    "#             labels_incorrect.append(labels[incorrect_pred].numpy())\n",
    "# print(f'Total no. of images in validation set: {total}')\n",
    "# print(f'Incorrectly classified images in validation set: {incorrect}')\n",
    "# inaccuracy = (incorrect / total) * 100        \n",
    "# print(f\"Inaccuracy: {inaccuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchinfo\n",
    "\n",
    "# torchinfo.summary(model, (64, 1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_without_head = list(model.children())[:-2]\n",
    "# print(model_without_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def freeze_unfreeze(nn_model, head_name, param_requires_grad=False):\n",
    "#     counter = 0\n",
    "#     for name, layer in nn_model.named_children():\n",
    "#         if name.lower() != head_name:\n",
    "#             for param in layer.parameters():\n",
    "#                 print(f\"{name} params with shape = {param.shape} are frozen\")\n",
    "#                 #param.requires_grad = param_requires_grad\n",
    "#                 counter += 1\n",
    "#     print(f\"Froze {counter} params\")                \n",
    "\n",
    "# freeze_unfreeze(model, \"fc\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0197751694b00855cd01780d565fa2e16f7945f624c4146f8d6aac863c2ba178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('fastai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
