{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1, padding=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, \n",
    "                    padding=padding, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        # The first conv layer downsamples the input if stride > 1. This happens at the begining\n",
    "        # of the 2nd, 3rd and 4th residual blocks. \n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # If the input has been downsampled by conv1 operation, correspondingly the input needs \n",
    "        # to be downsampled by the downsample operation before it can be added to the output of \n",
    "        # the residual block\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block : Type of block (BasicBlock or Bottleneck block)\n",
    "def _make_layer(block, in_channels, out_channels, blocks, stride=1):\n",
    "    downsample = None\n",
    "    # if the conv layer performs a downsample of input, we need a downsample function to perform an equivalent\n",
    "    # scaling of x.\n",
    "    if in_channels != out_channels or stride > 1:\n",
    "        downsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "    layers = []\n",
    "    # The first conv layer in the block that performs downsampling of the input\n",
    "    layers.append(block(in_channels, out_channels, stride, downsample))     \n",
    "    # the remaning layers in the block do no downsampling\n",
    "    in_channels = out_channels    \n",
    "    for _ in range(1, blocks):\n",
    "        layers.append(block(in_channels, out_channels))\n",
    "    return nn.Sequential(*layers)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "# The input image size is 224 * 224 * 3\n",
    "class ResNetLit(pl.LightningModule):\n",
    "    def __init__(self, block, layers, num_classes, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.layers = layers\n",
    "        self.num_classes = num_classes  \n",
    "        self.train_loss = nn.CrossEntropyLoss()\n",
    "        self.val_loss = nn.CrossEntropyLoss()\n",
    "        self.train_accuracy = torchmetrics.Accuracy() \n",
    "        self.val_accuracy = torchmetrics.Accuracy()\n",
    "        self.conv1_out_channels = 64     \n",
    "        # in=224, out=112\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=self.conv1_out_channels, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1_out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # in = 112, out = 56\n",
    "        # out = ((in - padding) / stride) + 1\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # The first layer with 3 residual blocks stacked. No downsampling happens in the first layer\n",
    "        # in = 56, out = 56, in_channels = out_channels = 64\n",
    "        self.layer1 = _make_layer(block, in_channels=self.conv1_out_channels, \n",
    "                                    out_channels=self.conv1_out_channels, blocks=layers[0])\n",
    "        # The second layer with 4 residual blocks. \n",
    "        # The first residual block dows downsampling directly by convolutional layers that have a stride of 2\n",
    "        # in = 56, out = 28, in_channels = 64, out_channels = 128\n",
    "        self.layer2 = _make_layer(block, in_channels=self.conv1_out_channels,\n",
    "                                    out_channels=self.conv1_out_channels*2, blocks=layers[1], stride=2)                                    \n",
    "        # The third layer with 6 residual blocks\n",
    "        # in = 28, out = 14, in_channels = 128, out_channels = 256 \n",
    "        self.layer3 = _make_layer(block, in_channels=self.conv1_out_channels*2,\n",
    "                                    out_channels=self.conv1_out_channels*4, blocks=layers[2], stride=2)\n",
    "        # The fourth layer with 3 residual blocks                                    \n",
    "        # in = 14, out = 7, in_channels = 256, out_channels = 512 \n",
    "        self.layer4 = _make_layer(block, in_channels=self.conv1_out_channels*4,\n",
    "                                    out_channels=self.conv1_out_channels*8, blocks=layers[3], stride=2)\n",
    "        # in = 7, out = 1                                       \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(in_features = self.conv1_out_channels*8, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        x = self.conv1(x)   # in: 224, out: 112\n",
    "        x = self.bn1(x)     \n",
    "        x = self.relu(x)    \n",
    "        x = self.maxpool(x) # in: 112, out: 56\n",
    "\n",
    "        x = self.layer1(x)  # in: 56, out: 56\n",
    "        x = self.layer2(x)  # in: 56, out: 28\n",
    "        x = self.layer3(x)  # in: 28, out: 14\n",
    "        x = self.layer4(x)  # in: 14, out: 7\n",
    "\n",
    "        x = self.avgpool(x) # in: 7, out: 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)  \n",
    "        return x    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        #loss = F.cross_entropy(y_pred, y).detach().item()        \n",
    "        loss = self.train_loss(y_pred, y)\n",
    "        acc = self.train_accuracy(y_pred, y).detach().item()\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"train_accuracy\", acc, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        loss = self.val_loss(y_pred, y).detach()        \n",
    "        acc = self.val_accuracy(y_pred, y).detach()\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"val_accuracy\", acc, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "data_path = \"./mnist_data\"\n",
    "data_transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                     transforms.Normalize([0.1307], [0.3081])])\n",
    "train_mnist = datasets.MNIST(data_path, train=True, download=True, transform=data_transform)\n",
    "val_mnist = datasets.MNIST(data_path, train=False, download=True, transform=data_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_mnist, batch_size=128, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_mnist, batch_size=128, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe443c523d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img, label = train_mnist[0]\n",
    "print(type(img))\n",
    "plt.imshow(img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name           | Type              | Params\n",
      "------------------------------------------------------\n",
      "0  | train_loss     | CrossEntropyLoss  | 0     \n",
      "1  | val_loss       | CrossEntropyLoss  | 0     \n",
      "2  | train_accuracy | Accuracy          | 0     \n",
      "3  | val_accuracy   | Accuracy          | 0     \n",
      "4  | conv1          | Conv2d            | 3.2 K \n",
      "5  | bn1            | BatchNorm2d       | 128   \n",
      "6  | relu           | ReLU              | 0     \n",
      "7  | maxpool        | MaxPool2d         | 0     \n",
      "8  | layer1         | Sequential        | 221 K \n",
      "9  | layer2         | Sequential        | 1.1 M \n",
      "10 | layer3         | Sequential        | 6.8 M \n",
      "11 | layer4         | Sequential        | 13.1 M\n",
      "12 | avgpool        | AdaptiveAvgPool2d | 0     \n",
      "13 | fc             | Linear            | 5.1 K \n",
      "------------------------------------------------------\n",
      "21.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.3 M    Total params\n",
      "85.134    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15c3701b0b546ea9aff3d2db67e9e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a00b8414724185b143d852c2bbac18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e119ae73765498facf770d2433f7e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165dc89a8b6a4e958ff0d88bb1c7771f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa9605243374704ab0c01868de984f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f5e4a27c9d46c9813aa984a5a512df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ff0a3c3ff942fa814648485a988c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e64aa67f6e54e178ff2a98b5eba359b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b432aeb25a344a4898f60b512e7b666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5fa07d6f8f4800937f94aa13c18412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68bf51700624cd78395b51c40fbf717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3542682d706b42dc95451454ca49b95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "mnist_model_lit = ResNetLit(ResidualBlock, layers=[3,4,6,3], num_classes=10, in_channels=1)\n",
    "tb_logger = pl.loggers.TensorBoardLogger(\"logs\", \"mnist_model\")\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"./model\", monitor=\"val_loss\", \n",
    "                                      filename=\"resnet34-{epoch:02d}-{val-loss:.2f}\")\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    auto_select_gpus=True,\n",
    "    progress_bar_refresh_rate=20,\n",
    "    max_epochs=10,\n",
    "    logger=tb_logger,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "trainer.fit(mnist_model_lit, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of images in validation set: 10000\n",
      "Incorrectly classified images in validation set: 102\n",
      "Inaccuracy: 1.02%\n"
     ]
    }
   ],
   "source": [
    "model = ResNetLit.load_from_checkpoint(\n",
    "    \"./model/resnet34-epoch=09-val-loss=0.00.ckpt\", \n",
    "    block=ResidualBlock,\n",
    "    layers=[3,4,6,3],\n",
    "    num_classes=10,\n",
    "    in_channels=1)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "incorrect = 0\n",
    "total = 0\n",
    "predicted_labels_incorrect = []\n",
    "labels_incorrect = []\n",
    "with torch.no_grad():\n",
    "    counter=0\n",
    "    for imgs, labels in val_loader:                \n",
    "        predicted_labels = torch.argmax(model(imgs), dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct_pred = predicted_labels == labels\n",
    "        incorrect_pred = ~correct_pred\n",
    "        num_incorrect_pred = incorrect_pred.sum()\n",
    "        incorrect += int(num_incorrect_pred)\n",
    "        if num_incorrect_pred > 0:\n",
    "            predicted_labels_incorrect.append(predicted_labels[incorrect_pred].numpy())\n",
    "            labels_incorrect.append(labels[incorrect_pred].numpy())\n",
    "print(f'Total no. of images in validation set: {total}')\n",
    "print(f'Incorrectly classified images in validation set: {incorrect}')\n",
    "inaccuracy = (incorrect / total) * 100        \n",
    "print(f\"Inaccuracy: {inaccuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0197751694b00855cd01780d565fa2e16f7945f624c4146f8d6aac863c2ba178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('fastai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
